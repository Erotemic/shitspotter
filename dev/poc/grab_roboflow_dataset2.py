from __future__ import annotations
import os
import xdev
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any
import ubelt as ub
import kwutil
from urllib.parse import urlparse
from roboflow import Roboflow
import rich
import pandas as pd

try:
    import kwcoco
except Exception as ex:  # pragma: no cover
    raise RuntimeError("kwcoco is required. pip install kwcoco") from ex


# ----------------------------
# Helper / utility functions
# ----------------------------

FMT_FOR_TYPE = {
    "semantic-segmentation": "coco-segmentation",
    "object-detection": "coco",
    "classification": "clip",
}


def _norm_version_id(v):
    if v is None:
        return None
    try:
        return int(v)
    except Exception:
        return v


@dataclass
class ProjectRecord:
    """Mutable record for a single Roboflow project version.

    Fields that are typically user-authored in the seed YAML:
      - workspace
      - project_id
      - version_id
      - name (optional)
      - project_type (optional, filled from API if missing)
      - orig_url (optional)
      - my_fork_id (optional)

    Fields that this helper fills/caches:
      - full_id ("{workspace}/{project_id}/{version_id}")
      - dl_dpath (download root on disk)
      - url (Universe project URL)
      - splits (mapping of split->coco path)
      - split_stats (basic kwcoco split_stats per split)
      - num_versions (optional from API when fetched)
    """

    workspace: str
    project_id: str
    version_id: Optional[int | str]

    # Optional user or API-populated fields
    name: Optional[str] = None
    project_type: Optional[str] = None
    orig_url: Optional[str] = None
    my_fork_id: Optional[str] = None
    notes: Optional[str] = None
    use: Optional[bool] = None

    # Autogenerated / cached
    full_id: Optional[str] = None
    dl_dpath: Optional[str] = None
    url: Optional[str] = None
    splits: Dict[str, str] = field(default_factory=dict)
    split_stats: Dict[str, Any] = field(default_factory=dict)
    size_stats: Dict[str, Any] = field(default_factory=dict)
    num_versions: Optional[int] = None

    # Non-serialized transient objects
    _rf_project: Any = field(default=None, repr=False, compare=False)
    _rf_version: Any = field(default=None, repr=False, compare=False)

    @classmethod
    def from_dict(cls, d: Dict[str, Any]) -> "ProjectRecord":
        """Create a ProjectRecord from a dict. """
        return cls(
            workspace=d.get('workspace'),
            project_id=d.get('project_id'),
            version_id=_norm_version_id(d.get('version_id')),
            name=d.get('name'),
            project_type=d.get('project_type'),
            orig_url=d.get('orig_url'),
            my_fork_id=d.get('my_fork_id'),
            full_id=d.get('full_id'),
            dl_dpath=d.get('dl_dpath'),
            url=d.get('url'),
            splits=d.get('splits', {}) or {},
            split_stats=d.get('split_stats', {}) or {},
            num_versions=d.get('num_versions'),
            use=d.get('use'),
            # size_stats intentionally not loaded from cache in old code; keep default {}
        )

    def to_dict(self) -> Dict[str, Any]:
        d = {
            'workspace': self.workspace,
            'project_id': self.project_id,
            'version_id': self.version_id,
            'name': self.name,
            'project_type': self.project_type,
            'orig_url': self.orig_url,
            'my_fork_id': self.my_fork_id,
            'full_id': self.full_id,
            'dl_dpath': self.dl_dpath,
            'url': self.url,
            'splits': self.splits,
            'split_stats': self.split_stats,
            'size_stats': self.size_stats,
            'notes': self.notes,
            'use': self.use,
            'num_versions': self.num_versions,
        }
        # Remove None values for a cleaner YAML
        return {k: v for k, v in d.items() if v is not None and v != {}}

    @staticmethod
    def _key_from_dict(d: Dict[str, Any]) -> str:
        ws = d.get('workspace')
        pid = d.get('project_id')
        vid = _norm_version_id(d.get('version_id'))
        return f"{ws}/{pid}/{vid}"

    @property
    def key(self) -> str:
        """Stable key for this record: <workspace>/<project_id>/<version_id>."""
        return ProjectRecord._key_from_dict(self.to_dict())

    def normalize(self, base_dpath: os.PathLike | str) -> None:
        """Populate derived identifiers/paths/urls. """
        self.version_id = _norm_version_id(self.version_id)
        if self.version_id is not None:
            self.full_id = f"{self.workspace}/{self.project_id}/{self.version_id}"
        else:
            self.full_id = f"{self.workspace}/{self.project_id}"
        if self.dl_dpath is None and self.full_id:
            self.dl_dpath = str((ub.Path(base_dpath) / self.full_id))
        if self.url is None:
            self.url = f"https://universe.roboflow.com/{self.workspace}/{self.project_id}/dataset/{self.version_id}"

    def ensure_api_objects(self, rf: Roboflow, allow_api: bool = True) -> None:
        """
        Ensure _rf_project, project_type, and (if needed) _rf_version are set.
        Mirrors _ensure_api_objects.
        """
        must_fetch_version = self._rf_version is None and self.version_id is None
        must_fetch_project = self._rf_project is None and (must_fetch_version or self.project_type is None)

        if must_fetch_project:
            if not allow_api:
                raise Exception('Assumed we would not need the API')
            ws = rf.workspace(self.workspace)
            self._rf_project = ws.project(self.project_id)

        if self._rf_project and self.project_type is None:
            self.project_type = getattr(self._rf_project, 'type', None)

        if must_fetch_version and self._rf_project is not None:
            if not allow_api:
                raise Exception('Assumed we would not need the API')
            self._rf_version = self._rf_project.version(self.version_id)
            try:
                self.num_versions = len(self._rf_project.versions())
            except Exception:
                pass

    def ensure_download(self, rf: Roboflow) -> None:
        """Download if the directory seems empty. Mirrors _ensure_download."""
        dl = ub.Path(self.dl_dpath).ensuredir()
        subdirs = [p for p in dl.ls() if p.is_dir()] if dl.exists() else []
        if subdirs:
            return

        if self._rf_version is None:
            ws = rf.workspace(self.workspace)
            self._rf_project = ws.project(self.project_id)
            self._rf_version = self._rf_project.version(self.version_id)
            try:
                self.num_versions = len(self._rf_project.versions())
            except Exception:
                pass

        ptype = self.project_type or getattr(self._rf_project, 'type', None) or 'object-detection'
        fmt = FMT_FOR_TYPE.get(ptype, 'coco')
        rich.print(f"[magenta]Downloading {self.full_id} as {fmt}...[/magenta]")
        with ub.ChDir(self.dl_dpath):
            _ = self._rf_version.download(fmt)  # NOQA

    def discover_splits(self) -> None:
        """Populate self.splits."""
        dl = ub.Path(self.dl_dpath)

        if self.project_type == 'classification':
            candidates = list(dl.glob('*/*/_annotations.coco.json'))
            if not candidates:
                print('process classification data')
                paths = [p for p in dl.glob('*/*') if p.is_dir()]
                for split_dpath in paths:
                    dset = kwcoco.CocoDataset.from_class_image_paths(split_dpath)
                    dset.reroot(absolute=True)
                    dset.fpath = split_dpath / '_annotations.coco.json'
                    dset.reroot(absolute=False)
                    dset.dump()

        candidates = list(dl.glob('*/*/_annotations.coco.json'))
        splits = {}
        for found in candidates:
            split_name = found.parent.name
            if split_name in {'train', 'test', 'valid'}:
                splits[split_name] = str(found)
        self.splits = splits

    def compute_basic_stats(self) -> None:
        """Populate self.split_stats."""
        self.split_stats = self.split_stats or {}
        for split, fpath in (self.splits or {}).items():
            if split in self.split_stats and self.split_stats[split]:
                continue
            try:
                basic = kwcoco.CocoDataset(fpath).basic_stats()
            except Exception as ex:
                basic = {"error": str(ex)}
            self.split_stats[split] = basic

        walker = xdev.DirectoryWalker(ub.Path(self.dl_dpath), show_progress=False)
        walker.build()
        size_stats = walker.stats(typed=False)
        assert size_stats
        self.size_stats = size_stats
        # walker.write_report(max_depth=0)


class RoboflowProjectCache:
    """
    Caches metadata and local paths for Roboflow datasets and can download on
    demand. Designed for iterative, interactive IPython use.

    Typical workflow:
        mgr = RoboflowProjectCache(
            base_dpath="/data/joncrall/dvc-repos/roboflow-repos",
            cache_path="/data/joncrall/dvc-repos/roboflow-repos/projects.yaml",
            rf_api_key=os.environ.get("ROBOFLOW_API_KEY"),
        )
        mgr.load_seed_yaml(seed_yaml_text=YOUR_SEED_YAML_STRING)
        mgr.process_all(download_if_missing=True)
        mgr.dump_cache()
        df = mgr.stats_dataframe()
    """

    def __init__(self, base_dpath: str, cache_path: str, rf_api_key: Optional[str] = None):
        self.base_dpath = ub.Path(base_dpath).ensuredir()
        self.cache_path = ub.Path(cache_path)
        self.rf_api_key = rf_api_key or os.environ.get('ROBOFLOW_API_KEY')
        if not self.rf_api_key:
            raise RuntimeError("ROBOFLOW_API_KEY is not set.")
        self.rf = Roboflow(api_key=self.rf_api_key)
        self.records: List[ProjectRecord] = []

    # ----------------------------
    # Loading / Saving cache
    # ----------------------------
    def load_cache_if_exists(self):
        if self.cache_path.exists():
            data = kwutil.Yaml.load(self.cache_path)
            self.records = [ProjectRecord.from_dict(d) for d in data]
            rich.print(f"[green]Loaded cached records from {self.cache_path}[/green]")
        else:
            rich.print(f"[yellow]No existing cache at {self.cache_path}[/yellow]")

    def load_seed_yaml(self, seed_yaml_text: Optional[str] = None):
        """Load/merge seed YAML into records. Existing cache values win unless
        the seed provides new fields (e.g., new projects).
        """
        seed = []
        if seed_yaml_text:
            seed = kwutil.Yaml.coerce(seed_yaml_text, backend='pyyaml')
        # If a cache exists, load it first so we can merge
        if self.cache_path.exists() and not self.records:
            self.load_cache_if_exists()

        # Normalize/augment seed rows that only specify a URL
        normalized_seed = []
        for row in seed:
            row = dict(row)  # shallow copy we can mutate
            # If key fields are missing, try to derive them from url / orig_url
            if not row.get("workspace") or not row.get("project_id") or row.get("version_id") is None:
                url_hint = row.get("url") or row.get("orig_url")
                if url_hint:
                    parsed = _parse_universe_url(url_hint)
                    # Only fill blanks; never overwrite user-provided values.
                    if parsed.get("workspace") and not row.get("workspace"):
                        row["workspace"] = parsed["workspace"]
                    if parsed.get("project_id") and not row.get("project_id"):
                        row["project_id"] = parsed["project_id"]
                    if "version_id" in parsed and row.get("version_id") is None:
                        row["version_id"] = parsed["version_id"]
            normalized_seed.append(row)
        seed = normalized_seed

        # Merge seed into records (add missing, update known)
        by_key = {r.key: r for r in self.records}
        row = seed[0]
        for row in seed:
            key = ProjectRecord._key_from_dict(row)
            if key in by_key:
                # Override the cache with the latest data.
                rec = by_key[key]
                for k, v in row.items():
                    setattr(rec, k, v)
            else:
                print(f'new uncached key={key}')
                self.records.append(ProjectRecord.from_dict(row))
        # Normalize IDs / paths
        for rec in self.records:
            rec.normalize(self.base_dpath)

    def print_updated_seed_yaml(self):
        seed_rows = DEFAULT_SEED_YAML
        seed_keys = set.union(*[set(row.keys()) for row in seed_rows])
        seed_keys -= {'id', 'full_id', 'project_id', 'workspace', 'version_id'}
        new_seed_rows = [ub.udict(rec.to_dict()) & seed_keys for rec in self.records]
        old_text = kwutil.Yaml.dumps(seed_rows)
        new_text = kwutil.Yaml.dumps(new_seed_rows)

        old_text = kwutil.Yaml.dumps(kwutil.Yaml.loads(old_text, backend='pyyaml'), backend='pyyaml')
        new_text = kwutil.Yaml.dumps(kwutil.Yaml.loads(new_text, backend='pyyaml'), backend='pyyaml')
        print('---')
        print(xdev.difftext(old_text, new_text, context_lines=100, colored=True))
        print('---')
        print(ub.hzcat([old_text, ' | ', new_text]))
        print('---')
        print(new_text)

    def dump_cache(self):
        serial = [rec.to_dict() for rec in self.records]
        self.cache_path.parent.ensuredir()
        text = kwutil.Yaml.dumps(serial)
        self.cache_path.write_text(text)
        rich.print(f"[cyan]Wrote updated cache to {self.cache_path}[/cyan]")

    # ----------------------------
    # Processing
    # ----------------------------
    def process_all(self, download_if_missing: bool = True, allow_api=True):
        for rec in self.records:
            try:
                self._process_one(rec, download_if_missing=download_if_missing,
                                  allow_api=allow_api)
            except Exception:
                print(f'ERROR IN rec={rec}')
                raise

    def _process_one(self, rec, download_if_missing: bool = True, allow_api=True):
        rec.normalize(self.base_dpath)
        self._ensure_api_objects(rec, allow_api=allow_api)

        if download_if_missing:
            self._ensure_download(rec)

        rec.discover_splits()
        rec.compute_basic_stats()

    # ----------------------------
    # Internals
    # ----------------------------
    def _ensure_api_objects(self, rec: ProjectRecord, allow_api=True):
        # Only hit API if we must (to infer type or download version)

        # TODO: could add a method to rec that asks if it missing critical
        # fields.
        must_fetch_version = rec._rf_version is None and rec.version_id is None
        must_fetch_project = rec._rf_project is None and (must_fetch_version or rec.project_type is None)
        if must_fetch_project:
            print(f'MUST FETCH PROJECT FOR {rec.full_id}')
            if not allow_api:
                raise Exception('Assumed we would not need the API')
            ws = self.rf.workspace(rec.workspace)
            rec._rf_project = ws.project(rec.project_id)
        if rec._rf_project and rec.project_type is None:
            rec.project_type = getattr(rec._rf_project, 'type', None)
        if must_fetch_version and rec._rf_project is not None:
            print(f'MUST FETCH VERSION FOR {rec.full_id}')
            if not allow_api:
                raise Exception('Assumed we would not need the API')
            rec._rf_version = rec._rf_project.version(rec.version_id)
            try:
                rec.num_versions = len(rec._rf_project.versions())
            except Exception:
                pass

    def _ensure_download(self, rec: ProjectRecord):
        dl = ub.Path(rec.dl_dpath).ensuredir()
        # Weak check: if no subdirs, assume not downloaded yet
        subdirs = [p for p in dl.ls() if p.is_dir()] if dl.exists() else []
        if not subdirs:
            if rec._rf_version is None:
                ws = self.rf.workspace(rec.workspace)
                rec._rf_project = ws.project(rec.project_id)
                rec._rf_version = rec._rf_project.version(rec.version_id)
                try:
                    rec.num_versions = len(rec._rf_project.versions())
                except Exception:
                    pass
            # and rec._rf_version is not None:
            ptype = rec.project_type or getattr(rec._rf_project, 'type', None) or 'object-detection'
            fmt = FMT_FOR_TYPE.get(ptype, 'coco')
            rich.print(f"[magenta]Downloading {rec.full_id} as {fmt}...[/magenta]")
            with ub.ChDir(rec.dl_dpath):
                dataset = rec._rf_version.download(fmt)  # NOQA

    # ----------------------------
    # Public convenience
    # ----------------------------
    def stats_dataframe(self) -> pd.DataFrame:
        rows = []
        for rec in self.records:
            for split, fpath in (rec.splits or {}).items():
                try:
                    basic = rec.split_stats.get(split)
                    if basic is None:
                        basic = kwcoco.CocoDataset(fpath).basic_stats()
                except Exception as ex:
                    basic = {"error": str(ex)}
                rows.append({
                    'full_id': rec.full_id,
                    'split': split,
                    **(basic or {}),
                })
        df = pd.DataFrame(rows)
        return df

    def build_summary_df(self):
        import pandas as pd
        rows = []
        for rec in self.records:
            row = {}
            coco_stats = pd.DataFrame(rec.split_stats).sum(axis=1).to_dict()
            row['full_id'] = rec.full_id
            row['size_bytes'] = rec.size_stats['size']
            row['project_type'] = rec.project_type
            row['files'] = rec.size_stats['files']
            row['size'] = xdev.byte_str(rec.size_stats['size'])
            row['n_anns'] = coco_stats.get('n_anns', 0)
            row['n_imgs'] = coco_stats.get('n_imgs', 0)
            row['use'] = rec.use
            rows.append(row)
        df = pd.DataFrame(rows)
        df = df.sort_values('project_type').reset_index()
        rich.print(df)
        total = df.drop(['project_type', 'full_id'], axis=1).sum()
        total['full_id'] = 'total'
        total['size'] = xdev.byte_str(total['size_bytes'])
        print(total)

        print("usable:")
        df = df[df['use'].apply(bool)]
        df = df.sort_values('project_type')
        rich.print(df)
        total = df.drop(['project_type', 'full_id'], axis=1).sum()
        total['full_id'] = 'total'
        total['size'] = xdev.byte_str(total['size_bytes'])
        print(total)

    def find_unknown_top_level_dirs(self):
        mgr = self
        # Find directories that aren't registered to clean up
        known_top_level_dirs = set()
        for rec in mgr.records:
            rel_path = ub.Path(rec.dl_dpath).relative_to(mgr.base_dpath)
            known_top_level_dirs.add(rel_path.parts[0])
        existing_tlds = set([p.name for p in mgr.base_dpath.ls() if p.is_dir()])
        unknown_tlds = (existing_tlds - known_top_level_dirs)
        print(f'unknown_tlds = {ub.urepr(unknown_tlds, nl=1)}')

    def process_kwcoco_files(self):
        usable = [rec for rec in self.records if rec.use]
        test_datasets = []
        for rec in usable:
            dsets = []
            for split, fpath in rec.splits.items():
                dset = kwcoco.CocoDataset(fpath)
                # TODO: custom per-dataset logic for handling categories
                dset.rename_categories({
                    name: 'poop' for name in dset.categories().name
                }, merge_policy='update')
                dset.reroot(absolute=True)
                dsets.append(dset)
            full_dset = kwcoco.CocoDataset.union(*dsets)
            from shitspotter.gather import build_code
            full_dset.fpath = ub.Path(rec.dl_dpath) / 'data.kwcoco.zip'
            full_dset.reroot(absolute=False)
            code = build_code(full_dset)
            full_dset.fpath = ub.Path(rec.dl_dpath) / f'data_{code}.kwcoco.zip'
            full_dset.dump(verbose=True)
            full_dset.reroot(absolute=True)
            test_datasets.append(full_dset)

        combo_dset = kwcoco.CocoDataset.union(*test_datasets)
        combo_dset.fpath = self.base_dpath  / 'data.kwcoco.zip'
        code = build_code(combo_dset)
        combo_dset.fpath = ub.Path(self.base_dpath) / f'data_{code}.kwcoco.zip'


def _parse_universe_url(url: str) -> Dict[str, Any]:
    """
    Parse Roboflow Universe URLs like:
      https://universe.roboflow.com/<workspace>/<project>
      https://universe.roboflow.com/<workspace>/<project>/dataset/<version>
    Returns a dict possibly containing: workspace, project_id, version_id.
    """
    out: Dict[str, Any] = {}
    if not url:
        return out
    try:
        path = urlparse(str(url)).path.strip("/")
        parts = path.split("/")
        # Expect at least <workspace>/<project>
        if len(parts) >= 2:
            out["workspace"] = parts[0]
            out["project_id"] = parts[1]
        # Optional version: .../dataset/<N>
        if len(parts) >= 4 and parts[2] == "dataset":
            try:
                out["version_id"] = int(parts[3])
            except Exception:
                pass
    except Exception:
        pass
    return out


# ----------------------------
# Example: Seed YAML string (copy/paste your list here in IPython)
# ----------------------------
DEFAULT_SEED_YAML = kwutil.Yaml.coerce(
    r"""
    - workspace: "han-bjamu"
      project_id: "poppopo"
      version_id: 2
      name: "poppopo Computer Vision Dataset"
      project_type: "semantic-segmentation"
      orig_url: https://universe.roboflow.com/han-bjamu/poppopo
      url: https://universe.roboflow.com/han-bjamu/poppopo/dataset/2
      use: True
      notes: "Good annotations. validation only. decent variety. all close up. inside and outside picks. Poops in crates. Lots on rocks."

    - workspace: pazini
      project_id: dog-poop-detection-uip1h
      version_id: 3
      project_type: instance-segmentation
      url: https://universe.roboflow.com/pazini/dog-poop-detection-uip1h/dataset/3
      use: True
      notes: "Indoor images. Pomeranian. Potty training it on a wewe pad. Some human legs. Looks to be motion capture images from a camera trap feed. Objects are small."

    - workspace: ncue-n5bis
      project_id: dog-poop-c15x4-qolmq
      version_id: 1
      name: "Ｄｏｇ　ｐｏｏｐ"
      project_type: "semantic-segmentation"
      orig_url: https://universe.roboflow.com/ncue-n5bis/dog-poop-c15x4
      my_fork_id: jon-crall/dog-poop-c15x4-qolmq
      url: https://universe.roboflow.com/ncue-n5bis/dog-poop-c15x4-qolmq/dataset/1
      use: False
      notes: "need to deal with EXIF before we can use, otherwise ok, has augmentation artifacts. Some outside. Good polygons. Lots of augmentation duplicates. Most close up, some far away."

    - workspace: jon-crall
      project_id: poop_segmentation-cxrst
      version_id: 1
      project_type: semantic-segmentation
      orig_url: https://universe.roboflow.com/cogmodel/poop_segmentation
      full_id: jon-crall/poop_segmentation-cxrst/1
      url: https://universe.roboflow.com/jon-crall/poop_segmentation-cxrst/dataset/1
      use: False
      notes: "had to fork and use my url to build a dataset. Has EXIF issues. Looks similar to the ncue-n5bis/dog-poop-c15x4-qolmq/dataset/1 but maybe better?"

    - url: https://universe.roboflow.com/new-workspace-j42am/softpoop/dataset/1
      use: True
      notes: |-
         Reaonable. Close up. A lot of rocks. Also has chinese characters and poops on them.
         Some on tile, some on grass, some in kenels.

    - url: https://universe.roboflow.com/poopyskt/diarrhoea/dataset/3
      use: False
      notes: |-
         an image has watermarks. Ambiguous annotations. (missing splotches).
         Mostly decent. Probably usable. Some images on paper with Chinese
         characters.

    - url: https://universe.roboflow.com/poopyskt/lackofwater/dataset/2
      use: False
      notes: |-
          very close up. Missing some annotations. No serious artifacts, but
          maybe has duplicates. A few incorrect polygons, perhaps from a SAM
          model? Mostly good annotations, probably usable, but disable on the
          initila pass.

    - workspace: project-oftnd
      project_id: dog-6eieq
      version_id: 2
      full_id: project-oftnd/dog-6eieq/2
      project_type: "object-detection"
      orig_url: https://universe.roboflow.com/project-oftnd/dog-6eieq
      url: https://universe.roboflow.com/project-oftnd/dog-6eieq/dataset/2

    - workspace: benito
      project_id: dog-excrements
      version_id: 2
      id: benito/dog-excrements/2
      project_type: "classification"
      url: https://universe.roboflow.com/benito/dog-excrements/dataset/2

    - workspace: cj-capstone
      project_id: dog-poop-13vwg
      version_id: 2
      project_type: object-detection
      url: https://universe.roboflow.com/cj-capstone/dog-poop-13vwg/dataset/2

    - workspace: dog-poop
      project_id: dog-poop-qdsbu
      version_id: 11
      my_fork_id: jon-crall/dog-poop-qdsbu-mnmzh
      project_type: object-detection
      url: https://universe.roboflow.com/dog-poop/dog-poop-qdsbu/dataset/11

    - workspace: dog-poop
      project_id: fake-dog-poop
      version_id: 4
      project_type: object-detection
      url: https://universe.roboflow.com/dog-poop/fake-dog-poop/dataset/4

    - workspace: dog-poop
      project_id: fake-dog-poop-2
      version_id: 1
      project_type: object-detection
      url: https://universe.roboflow.com/dog-poop/fake-dog-poop-2/dataset/1

    - workspace: ncue-uhqpj
      project_id: dog-poop-mmvam
      my_fork_id: jon-crall/dog-poop-mmvam-8jiev
      version_id: 3
      project_type: object-detection
      notes: "seems black and white for some reason. Manual cut/paste augmentation with speckle."
      url: https://universe.roboflow.com/ncue-uhqpj/dog-poop-mmvam/dataset/3

    - workspace: garrett-b
      project_id: dog-poop-health-id
      version_id: 1
      project_type: object-detection
      full_id: garrett-b/dog-poop-health-id/1
      url: https://universe.roboflow.com/garrett-b/dog-poop-health-id/dataset/1

    - workspace: eleven
      project_id: poop-yr3xd
      version_id: 2
      project_type: object-detection
      full_id: eleven/poop-yr3xd/2
      url: https://universe.roboflow.com/eleven/poop-yr3xd/dataset/2

    - url: https://universe.roboflow.com/han-bjamu/poop-p2f4z/dataset/1
      notes: human-poop
      use: False

    - workspace: garretts-workspace
      project_id: dog-poop-health-id-2
      version_id: 2
      project_type: classification
      full_id: garretts-workspace/dog-poop-health-id-2/2
      url: https://universe.roboflow.com/garretts-workspace/dog-poop-health-id-2/dataset/2

    - url: https://universe.roboflow.com/new-workspace-j42am/dp-4iarz/dataset/1
      use: True
      notes: |-
           Good annotations. Grass background. Mixed, leafs, close up. some
           pavement, usually top down, but some from angles. High quality.

    - url: https://universe.roboflow.com/petalifev3/petalife-stool-classification/dataset/3
      use: False
      notes: |-
          Annotation quality is mixed. Many missing annotations. Close up, but
          low res.  Different species seem to be included. Background is
          cropped out in some.

    - url: https://universe.roboflow.com/poop-parasite/dog-poop-with-blood-gsh4s/dataset/4

    - url: https://universe.roboflow.com/infocomm-computer-vision/detection-on-dog-poop-dfj8t/dataset/3

    - url: https://universe.roboflow.com/audrey-kim/object-dog-poo-health/dataset/3

    - url: https://universe.roboflow.com/morris-w9jrb/shit-ia1gc/dataset/1

    - url: https://universe.roboflow.com/wangdali/shit-wob83/dataset/4
      notes: generic poop

    - url: https://universe.roboflow.com/amplify/pet-waste-detection/dataset/1

    - url: https://universe.roboflow.com/test-iznis/dog-poo/dataset/1

    - url: https://universe.roboflow.com/poo-detection/pet-poop-classifier/dataset/5

    - url: https://universe.roboflow.com/kenzu-v8nqp/feces-classifier/dataset/4

    - url: https://universe.roboflow.com/usa-jhtpj/dog-excrements-golzu/dataset/3

    - orig_url: https://universe.roboflow.com/lihi-gur-arie/feces
      url: https://app.roboflow.com/jon-crall/feces-6fn6n/dataset/1

    - url: https://universe.roboflow.com/poops/poops/dataset/2

    - url: https://universe.roboflow.com/wangdali/shit-wob83/dataset/4

    - url: https://universe.roboflow.com/pic-zotwd/poop_proyect/dataset/1

    - url: https://universe.roboflow.com/poopdetector/messalert/dataset/2

    """
)


# ----------------------------
# Quick-start helper for IPython
# ----------------------------

def quick_start():
    """
    import sys, ubelt
    sys.path.append(ubelt.expandpath('~/code/shitspotter/dev/poc'))
    from grab_roboflow_dataset2 import *  # NOQA
    from grab_roboflow_dataset2 import _norm_version_id
    """
    seed_yaml_text = DEFAULT_SEED_YAML
    base_dpath = ub.Path("/data/joncrall/dvc-repos/roboflow-repos")
    cache_path = ub.Path("/data/joncrall/dvc-repos/roboflow-repos/projects.yaml")
    download_if_missing = True

    mgr = RoboflowProjectCache(base_dpath=base_dpath, cache_path=cache_path)
    self = mgr  # NOQA
    mgr.load_seed_yaml(seed_yaml_text=seed_yaml_text)

    mgr.process_all(download_if_missing=download_if_missing, allow_api=1)
    rec = self.records[-1]  # NOQA

    mgr.print_updated_seed_yaml()

    mgr.build_summary_df()

    mgr.dump_cache()
    return mgr

if __name__ == '__main__':
    """
    CommandLine:
        python ~/code/shitspotter/dev/poc/grab_roboflow_dataset2.py
    """
    quick_start()

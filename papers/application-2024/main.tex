% WACV 2024 Paper Template
% based on the CVPR 2023 template (https://media.icml.cc/Conferences/CVPR2023/cvpr2023-author_kit-v1_1-1.zip) with 2-track changes from the WACV 2023 template (https://github.com/wacv-pcs/WACV-2023-Author-Kit)
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION

\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
%\usepackage[review,algorithms]{wacv}      % To produce the REVIEW version for the algorithms track
%\usepackage[review,applications]{wacv}      % To produce the REVIEW version for the applications track
%\usepackage{wacv}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{wacv} % To force page numbers, e.g. for an arXiv version


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{comment}


\usepackage{url}            % simple URL typesetting
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}         % Can be removed after putting your text content
\usepackage[numbers]{natbib}
\usepackage{doi}

%% extra
\usepackage{listings}
\usepackage{amsmath} 
\usepackage{xcolor}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\wacvPaperID{*****} % *** Enter the WACV Paper ID here
\def\confName{WACV}
\def\confYear{2025}


\begin{document}

\title{ShitSpotter --- A Dog Poop Detection Algorithm and Dataset}

\author{Jonathan Crall\\
Kitware\\
{\tt\small jon.crall@kitware.com}
%{\tt\small erotemic@gmail.com}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
%\and
%Second Author\\
%Institution2\\
%First line of institution2 address\\
%{\tt\small secondauthor@i2.org}
}
\maketitle


\begin{comment}
    cd $HOME/code/shitspotter
    python -m shitspotter.cli.coco_annotation_stats $HOME/data/dvc-repos/shitspotter_dvc/data.kwcoco.json \
        --dst_fpath $HOME/code/shitspotter/coco_annot_stats/stats.json \
        --dst_dpath $HOME/code/shitspotter/coco_annot_stats
\end{comment}

%%%%%%%%% ABSTRACT
\begin{abstract}

%This work chronicles one researcher's un-funded journey to build a phone
%application that can detect dog poop in images, and make the data widely
%available as a benchmark dataset.
We introduce a new --- currently 42 gigabyte --- "living" dataset of phone images of
dog poop with manually drawn or AI-assisted polygon labels.
The collection and annotation of this data started in late 2020 and is planned
to continue indefinitely.

The most recent snapshot of dataset is made publicly available across three
different distribution methods: one centralized and two decentralized (IPFS and
BitTorrent).
We perform an analysis and experimental comparison of the trade-offs between
distribution methods and discuss the feasibility of each with respect to
sharing open scientific data.

A baseline vision transformer is trained to segment the objects of interest
under a grid of hyperparameters, and we evaluate their impact. The best model
achieves a pixelwise mean average precision of ~0.8ish. 
Model weights are made publicly available with the dataset. 

Code to reproduce experiments is hosted on GitHub.

%A phone application to detect poop with these models is being developed and 
%will be made freely available.

\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}

Applications of a computer vision system able to detect and localize poop in
images are numerous.
Automated waste disposal to keep parks and backyards clean.
A part of a system for using feces to monitor wildlife populations.
A warning system in smart-glasses to prevent people from stepping in poop.
The motivating use case is to develop a phone application that can help a dog
owner find their dog's poop in a leafy park so they can pick it up.
Many of these applications can be realized with modern object detection and
segmentation methods (cite modern object detection and segmentation methods)
and a large labeled dataset to train on.


In addition to enabling several applications, poop detection is an interesting benchmark problem. 
It it a reasonable simple problem with a narrower scope (just a single class),
suitable for exploring the capabilities of object detection models that focus
on a single labeled class while also containing non-trivial challenges like:
resolution (quality of camera, distance to the camera),
distractors (leafs, pine cones, dirt and mud),
occlusion (bushes, overgrown grass),
variation in appearance (old, new, healthy, sick).
Investigation into cases where this problem is difficult may provide insight
into how to better train object detection and segmentation networks.


\begin{figure}[h]
\centering
\includegraphics[width=0.49\textwidth]{/data/joncrall/dvc-repos/shitspotter_dvc/analysis/viz_three_images.jpg}
\caption[]{
    The "before/after/negative" process.
    The orange box highlights the location of the poop (note the
    actual annotation is a polygon) in the "before" image.
    In the "after" image, it is the same scene but the poop has been removed.
    The "negative" image is a nearby similar scene, potentially with a distractor.
}
\label{fig:ThreeImages}
\end{figure}

Towards these ends we introduce a dataset suitable for training poop detection
models in order to enable applications that require detecting or localizing
poop in images. In order to allow to assist with annotation we collect images
using a "before/after/negative" protocol as shown in \Cref{fig:ThreeImages}. 

Our goal is to train an algorithm to classify which pixels contain poop, and
which don't. We train a segmentation model to provide a baseline on how
difficult this problem is. Our models show strong performance, although there
are notable failure cases indicating this problem is difficult even for modern
computer vision algorithms.

In order to allow other researchers to improve on our results the dataset must
be accessible and at least one entity needs to be willing to host it.
Centralized methods are the typical choice and offer very good speeds, 
but they can be expensive for an individual, so it usually requires an
institution willing to host or payment for a hosting service,
can prone to outages and version control is not built in.
In contrast, decentralized methods allow volunteers to host data offers ability
to validate data integrity. 
This motivates us to compare and contrast centralized cloud services,
BitTorrent, and IPFS as mechanisms for distributing datasets.

% VGG2 face got removed.
% https://github.com/ox-vgg/vgg_face2/issues/52


Our contributions are:
1. A challenging new \textbf{open dataset} of images with polygon segmentations.
2. An experimental \textbf{evaluation of baseline training} methods.
3. An experimental \textbf{comparison of dataset distribution} methods.
4. \textbf{Open code and models}.
Related work is discussed at the end.

% https://gist.github.com/liamzebedee/4be7d3a551c6cddb24a279c4621db74c
% https://gist.github.com/liamzebedee/224494052fb6037d07a4293ceca9d6e7


\section{Dataset}

Our first contribution is the collection of a new open dataset consisting of
dog poop images in a mostly urban environment and primary from three specific
dogs. Indoor images and poop from other dogs are present in the dataset but
were encountered less often and thus are less frequent.

In addition to this description, details covering the motivation, composition,
collection, preprocessing, uses, distribution, and maintenance are provided in
a standardized datasheet \cite{gebru_datasheets_2021}.

\subsection{Dataset Collection Protocol}
The majority of the dataset was collected using the "before/after/negative"
protocol.
When we encountered a poop in the wild --- usually from our own dogs, but
sometimes from other dogs --- before we dispose of it we 
take a "before" picture containing the poop.
Then we pick up the poop and take an "after" picture of the same area. 
Then we find a nearby area or perhaps something else in the nearby environment
that might act as a confuser (e.g. pine cone or leaf) and take a third
"negative" picture.

The majority of images in the dataset follow this pattern.
However, there are exceptions.
The first 6 months of data collection only involved the "before/after" part of the protocol.
Sometimes the researcher failed or was unable to take the 2nd or 3rd image.
These cases where the before / after / negative protocol can be
programmatically this can be identified.

Additionally, the dataset also contains a few dozen (84) images contributed
by friends and family. These images are images mostly do not follow the
before/after protocol and are marked for use only in testing.



Challenges

\subsection{Dataset Annotation}

Images were annotated using labelme \cite{wada_labelmeailabelme_nodate}. 
The boundaries of 4386 annotated polygons are illustrated in \Cref{fig:AllPolygons}.
Most annotations were initialized using the Segment Anything Model (SAM)
\cite{kirillov_segment_2023} and a point prompt. 
All AI polygons were manually reviewed, many SAM polygons were adjusted, and in
some cases SAM did not work and manual annotations were faster or needed.
Regions with shadows seemed to cause SAM the most trouble, but there were other
failure cases. Unfortunately, there is no metadata to indicate which polygons
were manually created or done using AI.  However, the number of vertices may be
a reasonable proxy to estimate this, as SAM tends to have higher fidelity
boundaries. 

%Anecdotal note: SAM \cite{kirillov_segment_2023} worked well to automatically
%segment the poop, many of these needed adjustments, especially in regions of
%shadows, but there were cases that required a completely manual approach.
%Unfortunately a clean record of what cases these were does not exist. 

\begin{figure}[h]
\centering
\includegraphics[width=0.4\textwidth]{/home/joncrall/code/shitspotter/coco_annot_stats/annot_stat_plots/all_polygons.png}
\caption[]{
    All annotations drawn on a single images with 0.8 opacity to demonstrate
    the distribution in annotation location, shape, and size with respect to
    image coordinates.
}
\label{fig:AllPolygons}
\end{figure}

\subsection{Dataset Stats and Analysis}

% Number of images, annotations, and other stats.

The dataset currently contains 6648 images and 4386 annotations and has spanned
4 years. Data was captured over 4 years at a mostly uniformly rate.  Most data
is localized to parks and sidewalks in a small city.  Weather conditions varied
between snowy, sunny, rainy.  The distribution of seasons, time-of-day,
daylight, and capture rate is illustrated in \Cref{fig:TimeOfDayDistribution}.

Roughly 1/3 of the dataset has annotations due to the other 2/3 of the
images being taken in a way where the object of interest was removed from the
scene.

Some images contain more than one annotation, and some images contain zero annotations.
The number of annotations per images is illustrated in \Cref{fig:AnnotsPerImage}.
Many images do contain more than one poop, and this can be for several reasons:
    1. a single poop broke into multiple disjoint parts (the exact criteria for this is sometimes ambiguous), 
    2. two dogs pooped nearby each other (this happens frequently). 
    3. one or more dogs has pooped in the same area over some period of
       time (some cases can be difficult to determine if it is poop or dirt).

Almost all images have a width/height of 4032 x 3024 (which could be rotated
based on EXIF data) with 6 being 4008 x 5344 and one being 768 x 1024.

\begin{figure}[h]
\centering
\includegraphics[width=0.4\textwidth]{/home/joncrall/code/shitspotter/coco_annot_stats/annot_stat_plots/images_timeofday_distribution.png}
\caption[]{
    Distribution of the time of year and time of day each image was taken.
    For images with geolocation and timestamp (assuming the timezone is local
    or given and correct) we also estimate the amount of daylight as indicated
    by the color of each dot. While the majority of the images are taken in
    daylight, there are a sizable number of nighttime images.
}
\label{fig:TimeOfDayDistribution}
\end{figure}


\begin{figure}[h]
\centering
\includegraphics[width=0.4\textwidth]{/home/joncrall/code/shitspotter/coco_annot_stats/annot_stat_plots/anns_per_image_histogram_splity.png}
\caption[]{
    Histogram of the number of annotations per image. 
    Only 35\% (2346) of the images contain annotations, the other 65\% (4302)
    are known not to contain poop. Of these 4302 about half of them were taken
    directly after the poop was picked up, and the other half are pictures of a
    nearby location.
}
\label{fig:AnnotsPerImage}
\end{figure}


\section{Models}

Our second contribution is an evaluation of several trained models to serve as
a baseline.

We use the training and evaluation system of \cite{Greenwell_2024_WACV}, which
can be trained to predict heatmaps from polygons and can evaluate those
heatmaps on a pixelwise-level. 


Specifically, we use the file named train\_imgs5747\_1e73d54f.kwcoco.zip with
5747 images to train and vali\_imgs691\_99b22ad0.kwcoco.zip which contains 691
images is our experimental test set.
In this paper we do not make use of the contributor data


The baseline architecture is a variant of a vision-transformer \cite{dosovitskiy_image_2021,bertasius2021space,Greenwell_2024_WACV}.

Number of parameters.
Memory at train time.
Memory at predict time.
Model size on disk.

In all experiments, we use half-resolution images, which means most images have
an effective width/height of 2016 x 1512. The network is given a widow size of
416,416, which means that multiple windows are needed to predict on entire
images.

Our model is a 12 layer encoder backbone with 384 channels, and 8 attention
heads. 
The segmentation head is a 4 layer MLP using encoder features.
Loss is computed pixelwise using FocalLoss \cite{ross2017focal} with a small
downweighting of pixels towards the edge of the window.
The optimizer is AdamW, and we vary learning rate, weight decay, and
perturb-scale (note: weight decay combined with a non-zero perterb scale
implements the shrink perturb trick: \cite{ash_warm_starting_2020}).
We use a OneCycle learning rate scheduler with a cosine annealing strategy and
starting fraction of 0.3.
Our effective batch size is 24 with a real batch size of 2 and 12 accumulate
gradient steps.


All regions without an annotation were available at train time, but we used an
undersampling strategy to randomly choose equal numbers of negative windows as
there were positive windows (i.e. windows with annotations in them).

This used about 20-22 GB of the 24 GB available on the 3090 throughout
training.

%\textbf{Static Parameters}:

\subsection{Model Experiments}

\begin{comment}
    SeeAlso:
    ~/code/shitspotter/experiments/run_pixel_eval_pipeline.sh

    python ~/code/shitspotter/dev/poc/estimate_train_resources.py

\end{comment}

Our experiments are meant mean to provide a reasonable baseline, and not a
comprehensive evaluation of state of the art models on this dataset.

Exact scripts to reproduce these experiments are in the code repo.

After finding a reasonable performing starting point, we performed an ablation
over learning rate and regularization parameters. 
\begin{figure}[h]
\centering

\includegraphics[width=0.4\textwidth]{/data/joncrall/dvc-repos/shitspotter_expt_dvc/_shitspotter_evals/aggregate/plots/macro-plots-macro_01_c1edce/params/resolved_params.heatmap_pred_fit.trainer.default_root_dir/scatter_nolegend/macro_results_resolved_params.heatmap_pred_fit.trainer.default_root_dir_metrics.heatmap_eval.salient_AP_vs_metrics.heatmap_eval.salient_AUC_PLT02_scatter_nolegend.png}
\caption[]{
    The pixelwise AP and AUC of the top evaluated checkpoints.
    For each training run we ran evaluation on a subset of checkpoints that
    achieved low validation loss.
    Points with the same color represent checkpoints from the same training run
    with with the same hyperparameters.
}
\label{fig:apauc_scatter}
\end{figure}


\begin{figure}[h]
\centering
\includegraphics[width=0.4\textwidth]{/data/joncrall/dvc-repos/shitspotter_expt_dvc/_shitspotter_evals/aggregate/plots/macro-plots-macro_01_c1edce/params/resolved_params.heatmap_pred_fit.trainer.default_root_dir/box/macro_results_resolved_params.heatmap_pred_fit.trainer.default_root_dir_metrics.heatmap_eval.salient_AP_PLT04_box.png}
\caption{
    The range of AP values over the evaluated checkpoints.
}
\label{fig:ap_boxplot}
\end{figure}


\begin{figure}[h]
\centering
\includegraphics[width=0.4\textwidth]{/data/joncrall/dvc-repos/shitspotter_expt_dvc/_shitspotter_evals/aggregate/plots/macro-plots-macro_01_c1edce/params/resolved_params.heatmap_pred_fit.trainer.default_root_dir/macro_results_resolved_params.heatmap_pred_fit.trainer.default_root_dir_PLT05_table.png}
\caption{
    The table mapping short names to the default root directory.
    TODO: create a table that shows an overview of the varied hyperparameters.
}
\label{fig:scatter_legend}
\end{figure}

% Scatterplot for all experiments used in this work.
% file:///home/joncrall/data/dvc-repos/shitspotter_expt_dvc/_shitspotter_evals/full_aggregate/plots/macro-plots-macro_01_c1edce/params/resolved_params.heatmap_pred_fit.trainer.default_root_dir/scatter_nolegend/macro_results_resolved_params.heatmap_pred_fit.trainer.default_root_dir_metrics.heatmap_eval.salient_AP_vs_metrics.heatmap_eval.salient_AUC_PLT02_scatter_nolegend.png


\begin{table*}[t]
\centering
\begin{tabular}{llllrr}
\toprule
                       default\_root\_dir &      lr & weight\_decay & perterb\_scale &  salient\_AP &  salient\_AUC \\
\midrule
shitspotter\_scratch\_20240618\_noboxes\_v7 &  0.0001 &     0.000001 &      0.000003 &    0.832460 &     0.992721 \\
shitspotter\_scratch\_20240618\_noboxes\_v6 &  0.0001 &          0.0 &           0.0 &    0.822544 &     0.979457 \\
shitspotter\_scratch\_20240618\_noboxes\_v5 &  0.0001 &      0.00001 &           0.0 &    0.820307 &     0.967862 \\
shitspotter\_scratch\_20240618\_noboxes\_v4 &  0.0001 &     0.000001 &           0.0 &    0.815864 &     0.987229 \\
shitspotter\_scratch\_20240618\_noboxes\_v2 &  0.0003 &     0.000003 &      0.000001 &    0.810662 &     0.968507 \\
shitspotter\_scratch\_20240618\_noboxes\_v3 &   0.001 &      0.00001 &      0.000003 &    0.766480 &     0.989868 \\
shitspotter\_scratch\_20240618\_noboxes\_v8 &  0.0001 &     0.000001 &           0.0 &    0.745779 &     0.966297 \\
\bottomrule
\end{tabular}
\end{table*}


This restricted set is illustrated in \Cref{fig:scatter-subset}.

We evaluate each model with standard pixelwise segmentation metrics, where each
pixel is is considered as a binary classification example (poop-vs-background).
For each pixel the truth is compared to its predicted score, and we compute
standard metrics of average-precision (AP) and  area under the ROC curve (AUC)
\cite{powers_evaluation_2011}.

\Cref{fig:scatter-all} illustrates the AP and AUC of all baseline models trained.
These include ad-hoc parameters settings when searching for a stable training
configuration.

\begin{figure}[h]
\centering
\includegraphics[width=0.4\textwidth]{/data/joncrall/dvc-repos/shitspotter_expt_dvc/_shitspotter_evals/aggregate/plots/resources.png}
\caption[]{
    Time and energy resources used to perform model evaluation.

    Note: this table does not include training time, which was not measured
    directly at the time. Our stated train time estimates are based on.

}
\label{fig:resource}
\end{figure}


We trained 7 models from scratch with varied hyperparameters.

All models were trained on a single machine with an 11900k CPU and a 3090 GPU.

We measure the predict-time resources using codecarbon \cite{lacoste2019codecarbon} and report them in \Cref{fig:resource}.

Handling these measurements at train time is still in development, but we
estimate the time of each training run using timestamps on checkpoint and log
files.  Using indirect measurements based on timestamps of log and checkpoint
files, we estimate that each of the 7 training runs took about 5 days and 15
hours, totaling about 49.2 GPU days. Using the maximum 350W power draw of a
3090 GPU, we estimate energy usage of 17.2 kilowatt hours, assuming a 0.21
conversion ratio is 3.6 kilograms of CO2.

\begin{comment}
import kwutil.util_units

reg = kwutil.util_units.unit_registry()
gpu_power = 350 * reg.watt
time = 49.2 * reg.hour

co2kg_per_kwh = 0.210
energy_usage = (gpu_power *  time).to(reg.kilowatt * reg.hour)

co2_kg = energy_usage.m * co2kg_per_kwh
print(f'{round(co2_kg, 1)} CO2 kg')

dollar_per_kg = 0.015

cost_to_offset = dollar_per_kg * co2_kg
print(f'cost_to_offset = ${cost_to_offset:4.2f}')
\end{comment}


However, the experiments presented here were not the only ones performed in
determining the hyperparameters we held constant here. 

The path to the presented experiments involved trying over 42 training run with
a wider variation of parameters. In total we estimate the total GPU time as
158.9 days with an average of 3.75 days per run, which is emits roughly 280.4
CO2 kg with a cost of about \$4.21 to offset.




\begin{table*}[t]
\begin{tabular}{llllr}
\toprule
        node & resource &           total &            mean &  num \\
\midrule
heatmap\_eval & duration & 0 days 07:52:02 & 0 days 00:13:29 &   35 \\
heatmap\_pred & duration & 0 days 04:23:34 & 0 days 00:07:32 &   35 \\
heatmap\_pred &   co2\_kg &        0.667817 &         0.01908 &   35 \\
heatmap\_pred &      kwh &        3.172888 &        0.090654 &   35 \\
\bottomrule
\end{tabular}
\caption{All experiments table}
\end{table*}


\begin{table*}[t]
\begin{tabular}{llllr}
\toprule
        node & resource &           total &            mean &  num \\
\midrule
heatmap\_eval & duration & 4 days 14:37:02 & 0 days 00:20:07 &  330 \\
heatmap\_pred & duration & 6 days 06:52:11 & 0 days 00:27:31 &  329 \\
heatmap\_pred &   co2\_kg &       17.954668 &        0.054573 &  329 \\
heatmap\_pred &      kwh &       85.305086 &        0.259286 &  329 \\
\bottomrule
\end{tabular}
\caption{Limited experiments chosen for}
\end{table*}


\section{Distribution / Sharing}

% BitTorrent can be vulnerable to MITM:
% https://www.reddit.com/r/technology/comments/1dpinuw/south_korean_telecom_company_attacks_torrent/

Our third contribution is an exploration of distributed and centralized data distribution methods. 

Cloud storage for a modest amount of data can be expensive.

Decentralized methods can allow information to persist so long as at least 1
person has the data.

BitTorrent is a well known distributed system.

IPFS is a new similar tool \cite{benet_ipfs_2014,bieri_overview_2021}.


Discuss distributing the dataset via IPFS versus centralized distribution
systems.

Decentralized Method - IPFS and BitTorrent.
Centralized Method - Girder


The specific version of the dataset used in this paper was snap-shotted on
2024-07-03 and has the IPFS content ID of:
bafybeiedwp2zvmdyb2c2axrcl455xfbv2mgdbhgkc3dile4dftiimwth2y.

Observations:
\begin{itemize}
    \item IPFS via https using gateways does not always work well.
    \item IPFS usually works well if you use the CLI.
    \item IPFS is easier to update.
    \item IPFS does rehash every file, which induces an O(N) scalability constraint.
    \item IPFS does rehash every file, which induces an O(N) scalability constraint.
\end{itemize}


IPFS vs BitTorrent:

Both of which have the ability to use the Kademlia - distributed hash table (DHT) \cite{maymounkov_kademlia_2002}.
IPFS always uses its DHT, where as BitTorrent the Kademlia-based Mainline
Tracker can be disabled in favor of 3rd party trackers.

An excellent overview of protocols details can be found \cite{zebedee_comparing_2023}.
Our comparison is going to focus on measurements.

% Much of the 
% https://gist.github.com/liamzebedee/224494052fb6037d07a4293ceca9d6e7

%[Steiner, En-Najjary, Biersack 2022]

The Mainline Tracker is a DHT for bittorrent.

% See Also:
% Long Term Study of Peer Behavior in the KAD DHT
% https://git.gnunet.org/bibliography.git/plain/docs/Long_Term_Study_of_Peer_Behavior_in_the_kad_DHT.pdf
% We have been crawling the entire KAD network once a day for more than a year to track end-users with static
% IP addresses, which allows us to estimate end-user lifetime and the fraction of end-users changing their KAD ID.


% https://academictorrents.com/docs/about.html
Dataset (is / will be) tracked on Academic Torrents \cite{academic_torrents_Cohen2014}.


\subsection{Distribution Experiments}

Measure the performance of our algorithm versus a baseline.

Measure the speed of IPFS vs BitTorrent.

Time to create torrent vs time to pin to IPFS.

%-------------------------------------------------------------------------
\section{Related Work}

Object detection

Waste detection is an important problem but with relatively few open datasets.
%  https://paperswithcode.com/dataset/zerowaste
The ZeroWaste dataset \cite{bashkirova_zerowaste_2022} contains 1,800 segmented
video frames and 6000 unlabeled frames in a recycling facility.
% https://paperswithcode.com/dataset/taco
The TACO dataset \cite{proenca_taco_2020} is another "living" dataset
containing 1500 images with 4,784 annotations over 60 classes.
% TrashCan https://paperswithcode.com/dataset/trashcan
% Top Datasets on paperwith code
% https://paperswithcode.com/datasets?mod=images&task=semantic-segmentation&page=2


While ours is the largest publicly available poop dataset that we are aware of,
it is not the first.
A dataset of 100 dog poop images was collected and used to train a FasterRCNN
model in 2019 but this dataset and model are not publicly available \cite{neeraj_madan_dog_2019}.
The MSHIT dataset \cite{mshit_2020} consists of 3.89GB of real images with fake
poop (e.g. plastic poop) in controlled environments.
The company iRobot has a dataset of annotated indoor poop images used to train
Roomba j7+ to avoid collisions, but as far as we are aware, this is not
available \cite{roomba_2021}.

\section{Conclusion}

The ShitSpotter dataset is 42GB of images with polygon segmentations of dog
poop. 

We train and evaluate several baseline segmentation models, the best of which 
achieve an AP/AUC of ...

Our dataset is sufficient to train an object detection network to (level of
precision/recall).
Our experimental evaluation is limited by lack of model diversity, but it
serves as a baseline for future exploration.

We make data and models available over 3 distribution mechanisms: 
cloud storage, BitTorrent, and IPFS.

Decentralized methods are feasible methods of distribution, with strong
security but they can be slow.
IPFS is a promising solution for hosting scientific datasets, but does have pain points.
In contrast bittorrent can do X/Y/Z, but ...
Lastly centralized cloud storage can give the best speeds, but sacrifices some
security and can be less robust.

Directions for future research / development are:
1. Add lightweight object-level head and test object detection metrics.
2. Optimize model architectures for mobile devices.
3. Launch phone application.
4. Improve model / data distribution.


%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{citations}
}
%\bibliographystyle{unsrtnat}
%\bibliography{references}  %%% Uncomment this line and comment out the ``thebibliography'' section below to use the external .bib file (using bibtex) .

\end{document}

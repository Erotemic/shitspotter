\appendix

\section{Dataset}

\subsection{Additional Comparisons}
\label{sec:expanded_relatedwork}

In \Cref{sec:relatedwork} we compared to related work. Here we expand on this
by comparing our analysis plots. Every dataset is converted into the COCO
format and visualized using the same logic. \Cref{fig:compare_allannots}
visualizes the annotations of all datasets. We make similar visualizations 
for other comparable dataset metrics.
\Cref{fig:combo_anns_per_image_histogram_splity} shows the number of annotations per image.
\Cref{fig:combo_image_size_scatter} shows of image sizes in each dataset.
\Cref{fig:combo_obox_size_distribution_logscale} shows the distribution of width and heights of oriented bounding boxes fit to annotation polygons.
\Cref{fig:combo_polygon_area_vs_num_verts_jointplot} shows the area of each polygon versus the number of vertices (which could be used to estimate the likelihood a polygon was generated by AI for our dataset).
\Cref{fig:combo_polygon_centroid_relative_distribution} shows the distribution of centroid positions (relative to the image size).


\begin{figure*}[ht]
\centering
\includegraphics[width=1.0\textwidth]{plots/appendix/dataset_compare/combo_anns_per_image_histogram_splity.png.png}
\caption[]{
    Number of annotations per image in each dataset.
}
\label{fig:combo_anns_per_image_histogram_splity}
\end{figure*}


\begin{figure*}[ht]
\centering
\includegraphics[width=1.0\textwidth]{plots/appendix/dataset_compare/combo_image_size_scatter.png.png}
\caption[]{
    Image size distributions of each dataset. 
    Ours has two primary width/heights.
}
\label{fig:combo_image_size_scatter}
\end{figure*}


\begin{figure*}[ht]
\centering
\includegraphics[width=1.0\textwidth]{plots/appendix/dataset_compare/combo_obox_size_distribution_logscale.png.png}
\caption[]{
    Oriented bounding box size distributions (log10 scale) of each dataset.
}
\label{fig:combo_obox_size_distribution_logscale}
\end{figure*}

\begin{figure*}[ht]
\centering
\includegraphics[width=1.0\textwidth]{plots/appendix/dataset_compare/combo_polygon_area_vs_num_verts_jointplot_logscale.png.png}
\caption[]{
    Polygon area versus number of vertices (log10 scale) for each dataset.
    The polygons with more vertices are more likely to be AI generated.
}
\label{fig:combo_polygon_area_vs_num_verts_jointplot}
\end{figure*}

\begin{figure*}[ht]
\centering
\includegraphics[width=1.0\textwidth]{plots/appendix/dataset_compare/combo_polygon_centroid_relative_distribution.png.png}
\caption[]{
    Polygon centroid relative distribution for each dataset. It is interesting
    to note patterns in this data. For instance, the outline of a street can be
    seen in CityScapes. In Zero Waste you can see the conveyor belt. ImageNet
    is more uniform. Ours is Gaussian distributed. 
}
\label{fig:combo_polygon_centroid_relative_distribution}
\end{figure*}

\FloatBarrier


\subsection{Additional Information}
\label{sec:expanded_dataset}

In \Cref{sec:dataset} we provided an overview of several dataset statistics.
In this appendix we expand on that with additional plots.
The distribution of image pixel intensities is illustrated in \Cref{fig:spectra}.
The distribution of images collected over time is shown in \Cref{fig:images_over_time}.
The distribution of annotation location is shown in \Cref{fig:centroid_location_distri} and sizes is shown
  in \Cref{fig:annot_obox_size_dist} and \Cref{fig:annot_area_verts_distri}.


\begin{figure*}[ht]
\centering
\includegraphics[width=0.9\textwidth]{figures/spectra.png}
\caption[]{
    The ``spectra'' or histogram of the pixel intensities in the dataset. 
    The dataset RGB mean/std is $[117, 124, 100], [61, 59, 63]$. 
    High and low saturated values occur, but are included in the stats.
    This was run on the older 2024-07-03 snapshot.
}
\label{fig:spectra}
\end{figure*}


\begin{figure*}[ht]
\centering
\includegraphics[width=0.9\textwidth]{figures/appendix/images_over_time.png}
\caption[]{
    The number of images collected over time.
}
\label{fig:images_over_time}
\end{figure*}


\begin{figure*}[ht]
\centering
\begin{subfigure}[b]{0.4\textwidth}
 \includegraphics[width=\textwidth]{figures/appendix/polygon_centroid_absolute_distribution.png}
 \caption{Absolute pixel coordinates.}
 \label{fig:centroid_abs}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.4\textwidth}
 \includegraphics[width=\textwidth]{figures/appendix/polygon_centroid_relative_distribution.png}
 \caption{Relative image coordinates.}
 \label{fig:centroid_rel}
\end{subfigure}
\caption{The distribution of annotation centroids in terms of (a) absolute image coordinates and (b) relative image coordinates. The absolute centroid distribution is bimodal because some images are taken in landscape mode and other in portrait mode.}
\label{fig:centroid_location_distri}
\end{figure*}


\begin{figure*}[ht]
\centering
\begin{subfigure}[b]{0.4\textwidth}
  \includegraphics[width=\textwidth]{figures/appendix/obox_size_distribution_jointplot.png}
  \caption{Linear scale.}
  \label{fig:annot_obox_size_dist_linear}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.4\textwidth}
  \includegraphics[width=\textwidth]{figures/appendix/obox_size_distribution_logscale.png}
  \caption{Log10 scale.}
  \label{fig:annot_obox_size_dist_log}
\end{subfigure}
\caption{The distribution of annotation sizes as measured by an oriented bounding box fit to each polygon. (a) shows this plot on a linear scale and (b) show this plot on a log scale.}
\label{fig:annot_obox_size_dist}
\end{figure*}


\begin{figure*}[ht]
\centering
\includegraphics[width=1.0\textwidth]{figures/appendix/polygon_area_vs_num_verts_jointplot.png}
\caption[]{
    The distribution of polygon areas versus the number of vertices in the polygon boundary.
    The SAM model tends to produce polygons with a higher number of vertices
    than manually drawn ones.  For smaller polygons there are two peaks in the
    number of vertices histograms likely corresponding to pure-manual versus
    AI-assisted annotations.
}
\label{fig:annot_area_verts_distri}
\end{figure*}

\FloatBarrier


\section{Data Distribution \& Transfer}
\label{sec:distribution}

%Report training time, energy usage, and carbon footprint with details in supplemental materials.
\begin{comment}
import pint
reg = pint.UnitRegistry()
reg.define('CO2 = []')
reg.define('dollar = []')
kwh = reg.Unit('kilowatt/hour')
energy_cost = 0.16 * reg.dollar / (kwh)
emission_cost = 25 * reg.dollar / (1000 * reg.CO2 * reg.metric_ton)
energy = 1321.99 * kwh
emission = 277.612 * reg.CO2 * reg.kg
train = (energy * energy_cost + emission * emission_cost)

energy = 109.63 * kwh
emission = 23 * reg.CO2 * reg.kg
eval = (energy * energy_cost + emission * emission_cost)

train + eval
\end{comment}
  

%train$^{*}$ & time        & 158.95 days      &     3.78 days  &   42 \\
%train$^{*}$ & energy      & 1,316.07 kWh     &     31.34 kWh  &   42 \\
%train$^{*}$ & emissions   & 276.37 \cotwo kg & 6.58 \cotwo kg &   42 \\

%todo: train time resource usage for maskrcnn and vit, reacnknowledge
%limitation, break down results over each.


%In our context we are mainly concerned with making the data available.
%In other words, given a content identifier, how long does it take to programmatically access the data?
  
%For a comparison of IPFS and BitTorrent on the protocol level see \cite{zebedee_comparing_2023}.
%Another candidate system is a newer similar tool called IPFS (InterPlanetary File System)
%  \cite{benet_ipfs_2014, bieri_overview_2021}.
%To quote the authors:
%"IPFS could be seen as a single BitTorrent swarm, exchanging objects within one Git repository".
%All data down to the block level is content addressable and stored in a Merkle DAG, which can simplify data
%  versioning compared to using a torrent.


%The challenge lies in the fact that designing and documenting an experiment
%sufficiently for reproducibility requires substantial effort and is prone to
%error. We suggest that reducing the friction in accessing the necessary data
%could improve these success rates. Specifically, this involves codifying data
%download and preparation processes. Datasets that are available via decentralized
%and content-addressable are particularly advantageous, as they can guarantee
%the integrity of the data prevent the issue of dead URLs.

%Centralized data distribution has many advantages. It is fast and has low
%traffic overhead. However, it is prone to failure.  
%Cloud storage for a modest amount of data can be expensive.

%In contrast, Decentralized methods can allow information to persist so long as
%at least 1 person has the data.

%However, there are certain drawbacks of decentralized dataset distribution to
%consider. One significant limitation is the potentially substantial connection
%time required to link with peers, particularly when the data lacks a sufficient
%number of "seeders". Furthermore there needs to be a mechanism to connect to
%peers that can share the data.


%For our purposes we 

%%IPFS vs BitTorrent:
%For a comparison of IPFS and BitTorrent on the protocol level see
%\cite{zebedee_comparing_2023}. In our context we are mainly concerned with
%making the data available.

%the main metric we care about is how easy 

%Both IPFS and BitTorrent are both effectively
%content addressable at the dataset level, which makes them both appropriate for our use case.

%We
%care about accessing the data quickly in order to use it.  Thus, our comparison
%is going to focus on download-time measurements.
%Both of which have the ability to use the Kademlia - distributed hash table (DHT) \cite{maymounkov_kademlia_2002}.
%IPFS always uses its DHT, where as BitTorrent the Kademlia-based Mainline
%Tracker can be disabled in favor of 3rd party trackers.
% Overview and comparison of protocols via github gist:
% https://gist.github.com/liamzebedee/224494052fb6037d07a4293ceca9d6e7
% https://gist.github.com/liamzebedee/4be7d3a551c6cddb24a279c4621db74c
%[Steiner, En-Najjary, Biersack 2022]
% See Also:
% Long Term Study of Peer Behavior in the KAD DHT
% https://git.gnunet.org/bibliography.git/plain/docs/Long_Term_Study_of_Peer_Behavior_in_the_kad_DHT.pdf
% We have been crawling the entire KAD network once a day for more than a year to track end-users with static
% IP addresses, which allows us to estimate end-user lifetime and the fraction of end-users changing their KAD ID.

%Both BitTorrent (starting with the v2 protocol introduced in 2017 \cite{cohen_bittorrent_2017}) and IPFS have the capability to recognize when two torrents or content identifiers (CIDs) contain the same file. This enables seeders to provide files to downloaders of either torrent or CID, enhancing the availability and redundancy of the data.
%Both BitTorrent (as of 2017 in the v2 protocol \cite{cohen_bittorrent_2017})
%and IPFS can recognize that two torrents/CID include the same file and seeders
%can provide files to downloaders of the other.


%Additionally, storing data in the cloud can become prohibitively expensive,
%even for modest amounts of data. In contrast, decentralized methods allow
%information to persist as long as at least one individual retains the data.

%Discuss distributing the dataset via IPFS versus centralized distribution
%systems.
%Decentralized Method - IPFS and BitTorrent.
%Centralized Method - Girder

% BitTorrent can be vulnerable to MITM:
% https://www.reddit.com/r/technology/comments/1dpinuw/south_korean_telecom_company_attacks_torrent/

In \Cref{sec:dataset_transfer} we briefly presented a brief set of data distribution experiments.
Here, we provide more background detail, motivation, and discussion.

Empirical evidence suggests that a substantial proportion of scientific studies have low reproducibility
  rates, which has raised concerns across various disciplines \cite{baker_reproducibility_2016}.
Ideally, scientific research should be independently reproducible.
Despite higher success rates in computer science (up to 60\%) compared to other fields, there is still room for improvement
\cite{NEURIPS2019_c429429b, collberg2016repeatability, desai_what_2024}.
Addressing this issue requires not just better experimental documentation but also more reliable and
  accessible data distribution methods.
Specifically, this involves robustly codifying data download and preparation processes.


Centralized data distribution methods allow for codified data access by storing URLs that point to datasets
  within the code, offering fast and direct access.
However, this approach lacks robustness.
It can fail if the provider goes offline, changes the URL, or stops hosting the data.
Additionally, cloud storage can be expensive, and users must trust that the provider delivers the correct
  data --- a risk that can be mitigated by using checksums to verify data integrity.
  %though this adds an extra
  %step for experiment designers.

In contrast, decentralized methods allow users to access data in the same way, even if the organization
  hosting the data changes.
%offer greater data longevity, accessibility, and integrity.
By leveraging content-addressable storage, where the dataset checksum acts as both the key to locate and
  validate the data, these methods ensure data integrity and nearly eliminate the risk of dead URLs, provided
  that at least one peer retains the data.
While decentralized systems face challenges such as longer connection times, increased network overhead, and
  the need for a robust peer network, their ability to ensure data access via a static address
  motivates our investigation

Specifically, we focus on two prominent candidates:
BitTorrent and IPFS.
BitTorrent \cite{cohen_incentives_2003, cohen_bittorrent_2017} is a well known sharing protocol that
  originally relied on centralized trackers and databases of torrent files to connect peers.
While trackers and torrent files are still prominent, torrents can be published to a distributed hash table
  (DHT) using the Kademlia algorithm \cite{maymounkov_kademlia_2002}.
This makes it an strong candidate for a decentralized distribution mechanism.
On the other hand, IPFS (InterPlanetary File System) \cite{benet_ipfs_2014, bieri_overview_2021} is a newer
  tool directly build directly on a DHT.
IPFS has been likened to ``a single BitTorrent swarm, exchanging objects within one Git repository''.
%All data down to the block level is content addressable and stored in a Merkle DAG, which can simplify data
%  versioning compared to using a torrent.
%However, both IPFS and BitTorrent are effectively content addressable at the dataset level, which makes them
%  both appropriate for our use case where we seek a static address that can be used to robustly access data.
Both IPFS and BitTorrent are content addressable at the dataset level, which makes them both appropriate for
  our use case where we seek a static address that can be used to robustly access data.

It is worth noting that git-based \cite{chacon2014progit} systems like
  HuggingFace~\cite{huggingface_datasets} with large file storage do gain some decentralized
  properties via multiple remotes, but not content identifiers.

For practitioners, key concerns are how quickly and reliably data can be accessed.
By comparing decentralized and centralized mechanisms access times for our dataset, we aim to make
  explicit the tradeoffs between the methods and inform decisions on adopting an approach.

%identify the most effective method for
%  ensuring that scientific datasets remain accessible and reproducible over time, thereby contributing to
%  improved reproducibility in scientific research


 
\FloatBarrier

\subsection{Data Distribution Discussion}
\label{sec:datset_discuss}

To assess the effectiveness of each mechanism we programmatically download our 42GB dataset and measure the
  time required to complete the transfer.
Each experiment was run five times, machines we controlled were separated by $\sim\!30$ kilometers with an
  average ping time of 48.48 ms.
For each test, we log transfer start and end times along with notes and code (provided in code repo).

While our measurements provide a reasonable estimate of for access time for each mechanism, there are
  notable limitations in our methodology.
First, different machines and networks have different upload and download speeds, and network congestion is
  variable.
For decentralized methods, we lack an automated mechanism separate peer-connection time and actual download
  time.
Additionally, Girder and HuggingFace required data to be packed into compressed archives, improving transfer
  efficiency due to fewer file boundaries.
In decentralized cases, we provide granular access to each file in the dataset, which avoids an extra
  unpacking step and enables sharing of the same file between different versions of the datasets and simpler
  updates, but decreases transfer efficiency.
Due to this, we provide both a compressed and uncompressed rsync baseline.
Another confounding factor is that with decentralized mechanisms the number of seeders is not controlled
  for.
Subsets of the data have been hosted on IPFS for years, and portions of the dataset may be provided by
  unknown members of the network.
For BitTorrent, our initial transfers only had one seeder, but during our tests other nodes accessed and
  started to provide the data.

Despite significant testing limitations, our measurements quantify the expected data-access time penalty to
  gain the advantages of decentralized mechanisms.
With these limitations acknowledged, we present the transfer times statistics in \Cref{tab:transfertime}.
Alongside these measurements, several observations are worth noting.
Transferring files using IPFS had significantly delayed peer discovery times, and we were only able to
  connect two machines after manually informing them of each other's peer ID.
For BitTorrent, were unable to use the mainline DHT and fell back to using trackers.
We believe these peer discovery issues are because the dataset has a small number of seeders.
To test this, we downloaded other established datasets via IPFS and BitTorrent and found that the peer
  discovery time was almost immediate, suggesting that this becomes less of an issue as a dataset is shared.
However, the inability to quickly find a nearby peer is a major issue for initial or private dataset
  development.


\subsection{Dataset Versions}

An advantage of content identifiers is that they are resistant to link rot as long as at least one peer
  hosts the data, and more importantly, they can never resolve to the wrong content.
This makes them highly attractive for scientific reproducibility.
In this work we relied on two main dataset versions, each specified by a stable content-based identifiers:

\paragraph{Version from 2024-07-03}
\begin{itemize}
  \item IPFS CID: \ipfscid{bafybeiedwp2zvmdyb2c2axrcl455xfbv2mgdbhgkc3dile4dftiimwth2y}
  \item BitTorrent: \magnetlink{ee8d2c87a39ea9bfe48bef7eb4ca12eb68852c49}
\end{itemize}

\paragraph{Version from 2025-04-20}
\begin{itemize}
  \item IPFS CID: \ipfscid{bafybeia2uv3ea3aoz27ytiwbyudrjzblfuen47hm6tyfrjt6dgf6iadta4}
  \item BitTorrent: \magnetlink{27a2512ae93298f75544be6d2d629dfb186f86cf}
\end{itemize}
Note: the hash suffix of the magnet URL can be searched on \url{academictorrents.com}.

At the time of writing, the version of the dataset on HuggingFace is the latest, and we use git tags that
  correspond with the date of release and the IPFS CID to help identify dataset versions.
However, unlike the decentralized methods, these are not guaranteed to point to the expected version of the
  dataset.
At the time of writing the HuggingFace URL is:
\url{https://huggingface.co/datasets/\redact{erotemic}/scatspotter} and the Girder URL is:
\url{https://data.\redact{kitware}.com/?#user/598a19658d777f7d33e9c18b/folder/66b6bc7ef87a980650f41f98}.


\section{Model \& Training Details}
\label{sec:experiment_details}

In \Cref{sec:models} we provided our main results.
However, a key limitation of these results is the imbalance between model types, with 42 of 47 trained
  models being VIT-ssegs, 2 MaskRCNN models, 2 YOLO models, and 1 tuned GroundingDino model.
Future work could further optimize MaskRCNN, Grounding~DINO, YOLO, and other models to improve both
  performance and comparability, but these results are enough to establish a useful baseline.

For non-VIT models we adhered as closely as possible to the default parameters of their respective
  frameworks, applying changes needed to support our generalized KWCoco format and to fit on a single GPU.
For complete details, we provide links to the training and evaluation scripts for each model family (see
  below).
A docker image with dependencies pre-installed is also available:
\dockerimage
  {https://hub.docker.com/layers/\redact{erotemic}/shitspotter/latest/images/sha256-aec306e515a5c8bef162c872c96b6a82ff3f4798f4b796f1431ce8f1f6288747}
  {\redact{erotemic}/shitspotter:latest}.

\paragraph{Experiment scripts (by task).}

\begin{itemize}
  \item \textbf{Grounding~DINO:} \repolink{experiments/grounding-dino-experiments}{./experiments/grounding-dino-experiments}

  \item \textbf{YOLO-v9:} \repolink{experiments/yolo-experiments}{./experiments/yolo-experiments}

  \item \textbf{MaskRCNN:} \repolink{experiments/detectron2-experiments}{./experiments/detectron2-experiments}

  \item \textbf{VIT-sseg:} \repolink{experiments/geowatch-experiments}{./experiments/geowatch-experiments}
\end{itemize}


\subsection{Grounding Dino}

GroundingDINO was evaluated in two modes.
The zero-shot setting used the \hflink{IDEA-Research/grounding-dino-tiny} model from HuggingFace, applied
  directly to our validation and test splits with a fixed set of 10 prompts.
The tuned variant used the community \ghlink{longzw1997/Open-GroundingDino} implementation, initialized from
  \DINOPretrained{} with a BERT text encoder on a single GPU.
Training data was converted to ODVG JSONL format, and the label set reduced to two classes (``poop'' and
  ``unknown'').

The preprocessing pipeline resized the shorter side of each image to 800 pixels while maintaining aspect
  ratio, corresponding to a median scale factor of $\sim$0.26 for our dataset.

In the zero-shot evaluation, we tested ten prompts and selected ``animalfeces'' based on the highest
  validation Box-AP.
\Cref{tab:prompt_variations} shows the full ablation, illustrating that prompt choice strongly affects
  performance.
Prompt choice has a large effect, and the best prompt differs between validation and test splits, but
  overall zero-shot results remain low.


%Grounding DINO zero-shot prompt ablation:
%In \Cref{sec:models} we performed a zero-shot detection experiment with Grounding Dino. This required us to choose a prompt. We evaluated 10 possible choices on the validation set and chose the one with the highest Box AP. Results each variation is given in \Cref{tab:prompt_variations}.

\newcommand{\CaptionPrompt}{
\caption{Zero-shot detection results with varied prompts. The chosen prompt has a significant impact on scores, and the best prompt is different between validation and test datasets, but overall zero-shot results are all low scoring. Because this is a zero-shot setting, the validation set can be compared to the test set. Interestingly, the validation scores significantly lower than the test scores indicating a greater degree of difficulty. }
\label{tab:prompt_variations}
}


\begin{table*}[t]
\ifwacv \else \CaptionPrompt \fi
\centering
\begin{tabular}{lllllllll}
\toprule
\multicolumn{1}{l}{} & \multicolumn{4}{c}{Validation (n=691)} & \multicolumn{4}{c}{Test (n=121)} \\
 Prompt      & \makecell{AP\\Box}   & \makecell{AUC\\Box}   & \makecell{F1\\Box}   & \makecell{TPR\\Box}   & \makecell{AP\\Box}   & \makecell{AUC\\Box}   & \makecell{F1\\Box}   & \makecell{TPR\\Box}   \\
\midrule
 stool       & 0.01                 & 0.03                  & 0.07                 & 0.07                  & 0.05                 & 0.08                  & 0.18                 & 0.13                  \\
 droppings   & 0.02                 & 0.10                  & 0.14                 & 0.23                  & 0.08                 & 0.14                  & 0.27                 & 0.30                  \\
 petwaste    & 0.04                 & 0.14                  & 0.15                 & 0.25                  & 0.20                 & 0.25                  & 0.35                 & 0.34                  \\
 poop        & 0.04                 & 0.10                  & 0.17                 & 0.16                  & 0.17                 & 0.18                  & 0.31                 & 0.26                  \\
 dogpoop     & 0.05                 & 0.16                  & 0.17                 & 0.20                  & 0.24                 & 0.28                  & 0.38                 & 0.39                  \\
 caninefeces & 0.05                 & 0.16                  & 0.18                 & 0.29                  & 0.17                 & 0.24                  & 0.37                 & 0.39                  \\
 turd        & 0.05                 & 0.18                  & 0.18                 & 0.22                  & \textbf{0.27}        & \textbf{0.32}         & 0.39                 & 0.35                  \\
 feces       & 0.06                 & 0.21                  & 0.18                 & 0.27                  & 0.16                 & 0.26                  & 0.32                 & 0.39                  \\
 excrement   & 0.07                 & \textbf{0.22}         & 0.20                 & 0.28                  & 0.25                 & 0.31                  & 0.39                 & \textbf{0.42}         \\
 dogfeces    & 0.07                 & 0.21                  & \textbf{0.20}        & \textbf{0.31}         & 0.23                 & 0.29                  & \textbf{0.40}        & 0.38                  \\
 animalfeces & \textbf{0.08}        & 0.21                  & 0.20                 & 0.25                  & 0.23                 & 0.30                  & 0.39                 & 0.38                  \\
\bottomrule
\end{tabular}
\ifwacv \CaptionPrompt \fi
\end{table*}


\subsection{YOLO-v9}

YOLO-v9 experiments were based on the community \ghlink{WongKinYiu/yolov9} implementation.
We trained both pretrained (ImageNet-initialized) and from-scratch variants, using our fork adapted for
  KWCoco input.

All images were resized to 640$\times$640, corresponding to a median scale factor of $\sim$0.16.
Training used a batch size of 16 with batch accumulation set to 50, for an effective batch size of 800.
Optimization used AdamW with learning rate $3\times10^{-4}$ and weight decay of 0.01.
The pretrained runs started from \YOLOPretrained{}.


\subsection{MaskRCNN}

MaskRCNN experiments were run using a Detectron2 fork with KWCoco support.
Both pretrained and from-scratch models used the standard \texttt{R\_50\_FPN\_3x.yaml} configuration,
  differing only in initialization:
pretrained models used \MaskRCNNPretrained{}, while the from-scratch models started randomly.

To fit training on a single GPU, we reduced the learning rate to $2.5\times10^{-4}$, set the batch size to
  2, and trained for a maximum of 120{,}000 iterations.
The maximum image dimension was capped at 1024, giving a median scale factor of $\sim$0.25 for our
  dataset.


\subsection{VIT-sseg}
\label{sec:vit_models}

This subsection provides additional details on VIT-sseg models, which were the first architecture we
  explored for this problem and therefore have the most extensive analysis compared to other networks.

To train VIT-sseg models we use the training, prediction, and evaluation system presented in
  \cite{Greenwell_2024_WACV, crall_geowatch_2024}, which utilizes polygon annotations to train a pixelwise
  binary segmentation model.

In all experiments, we use half-resolution images, which means most images have an effective width $\times$
  height of 2,016 $\times$ 1,512.
We employ a spatial window size of 416 $\times$ 416 for network inputs, which means that multiple windows
  are needed to predict on entire images.
During prediction, we apply a window overlap of 0.3 with feathered stitching to prevent boundary artifacts.

To address the class imbalance in our dataset (where positives are patches containing annotations and
  negatives contain no annotations), we adopt a balanced sampling strategy.
Each ``epoch'' consists of randomly sampling 32,768 patches from the dataset with replacement, ensuring
  roughly equal numbers of positive and negative samples.
We train each network for 163,840 gradient steps.
For data augmentation we use random crops and flips.

Our baseline architecture is a variant \cite{bertasius2021space,Greenwell_2024_WACV} of a vision-transformer
  \cite{dosovitskiy_image_2021}.
The model is a 12-layer encoder backbone with 384 channels and 8 attention heads that feeds into a 4-layer
  MLP segmentation head.
It has 25,543,369 parameters and a size of 114.19 MB on disk.
At predict time it uses 1.96GB of GPU RAM.

We compute loss pixelwise using Focal Loss \cite{ross2017focal} with a small downweighting of pixels towards
  the edge of the window.
Our optimizer is AdamW \cite{loshchilov_decoupled_2018}, and we experiment with varying learning rate,
  weight decay, and perturb-scale (implementing the shrink perturb trick~\cite{ash_warm_starting_2020,dohare_loss_2023}).
We employ a OneCycle learning rate scheduler \cite{smith2019super} with a cosine annealing strategy and
  starting fraction of 0.3.
Our effective batch size is 24 with a real batch size of 2 and 12 accumulate gradient steps.
This setup consumes approximately 20 GB of GPU RAM during training.

\subsubsection{VIT-sseg Model Experiments}

To establish a baseline, we evaluated 35 training runs where we varied input resolutions, window sizes,
  model depth, and other parameters.
Although this initial search was somewhat ad-hoc, it provided insights into the optimal configuration for
  our model.
Building on the best hyperparameters from this search, we performed a sweep over 7 combinations of learning
  rate, weight decay, and perturb scale (i.e., shrink and perturb
  \cite{ash_warm_starting_2020,dohare_loss_2023}).
Scripts used to reproduce these experiments, as well as a log of the ad-hoc experiments, are available in
  the code repository.
Additionally, trained models are packaged and distributed with information about their training
  configuration.

Note:
the test dataset used in this appendix section is an older 30 image version with suffix {\tt d8988f8c},
  which is a subset of the more recent 121 image test set used in the main paper.


\newcommand{\VitResultCaption}{
\caption{
Results for the best-performing models on the validation set across 7 hyperparameter configurations.
The table provides detailed information about each configuration, including:
1) Configuration name (first column): a unique code identifying each training run used in the score scatter and box plots.
2) Varied hyperparameters (next three columns): specific values for learning rate, weight decay, and perturb scale that were used in each run.
3) Validation set performance (AP and AUC scores): metrics evaluating the model's performance on the validation set.
4) Test set performance (AP and AUC scores): metrics evaluating the model's performance on the test set using the same validation-maximizing models.
Note that the top AP score over all models on the test set was 0.65, but it did not correspond to one of these validation runs used for model selection.
Qualitative examples illustrating the performance of the top-scoring validation model listed here are provided in \cref{fig:test_heatmaps_with_best_vali_model}.
}
\label{tab:parameters_and_results}
}


\begin{table*}[t]
\ifwacv \else \VitResultCaption \fi
\centering
\begin{tabular}{llllllll}
\toprule
            \multicolumn{4}{l}{} & \multicolumn{2}{r}{Validation (n=691)} & \multicolumn{2}{r}{Test (n=30)} \\
Config Name  &   LR & Weight Decay & Perterb Scale & \makecell{AP\\Pixel} & \makecell{AUC\\Pixel} &  \makecell{AP\\Pixel} &  \makecell{AUC\\Pixel} \\
\midrule
        \textcolor[HTML]{623682}{D05} & 1e-4 &   1e-6 &  3e-6 & \textbf{0.7802} & \textbf{0.9943} &          0.5051 &          0.9125 \\
        \textcolor[HTML]{df8020}{D03} & 1e-4 &   1e-5 &  3e-7 &          0.7758 &          0.9707 &          0.4346 &          0.8576 \\
        \textcolor[HTML]{87b787}{D04} & 1e-4 &   1e-7 &  3e-7 &          0.7725 &          0.9818 &          0.4652 &          0.7965 \\
        \textcolor[HTML]{207fdf}{D02} & 1e-4 &   1e-6 &  3e-7 &          0.7621 &          0.9893 & \textbf{0.5167} & \textbf{0.9252} \\
        \textcolor[HTML]{20df20}{D00} & 3e-4 &   3e-6 &  9e-7 &          0.7571 &          0.9737 &          0.4210 &          0.7766 \\
        \textcolor[HTML]{df20df}{D01} & 1e-3 &   1e-5 &  3e-6 &          0.7070 &          0.9913 &          0.4607 &          0.9062 \\
        \textcolor[HTML]{b00403}{D06} & 1e-4 &   1e-6 &  3e-8 &          0.6800 &          0.9773 &          0.4137 &          0.8157 \\
        
\bottomrule
\end{tabular}
\ifwacv \VitResultCaption \fi
\end{table*}

\begin{comment}
    SeeAlso:
    ~/code/shitspotter/experiments/geowatch-experiments/run_pixel_eval_on_vali_pipeline.sh
    python ~/code/shitspotter/dev/poc/estimate_train_resources.py
\end{comment}

For each of the 7 hyperparameter combinations, we trained the model for 163,840 optimizer steps using a
  batch size of 24.
We defined an ``epoch'' as 1,365 steps, at which point we saved a checkpoint, evaluated validation loss, and
  adjusted learning rates.
To conserve disk space, we retained only the top 5 lowest-validation-loss checkpoints (although training
  crashes and restarts sometimes resulted in additional checkpoints, which are included in our evaluation).

Using the top-checkpoints, we predicted heatmaps for each image in the validation set.
We then performed binary classification on each pixel (poop-vs-background) using a threshold.
Next, we rasterized the truth polygons.
The corresponding truth and predicted pixels were accumulated into a confusion matrix, allowing us to
  compute standard metrics such as precision, recall, false positive rate, etc.
\cite{powers_evaluation_2011} for the specific threshold.
By sweeping a range of thresholds, we calculated the average precision (AP) and the area under the ROC curve
  (AUC).
We computed all metrics using scikit-learn \cite{scikit-learn}.
Due to the high number of true negative pixels, we preferred AP as the primary measure of model quality.
  
The details of the top model for each run, along with relevant hyperparameters, are presented in
  \Cref{tab:parameters_and_results}.
This table also includes the results on the small, held out, test set for the top model.

The results show strong performance on the validation set, with a maximum AP of $0.78$.
However, while the test AP for this model is good, it is significantly lower at $0.51$.
To investigate this discrepancy, we turned to qualitative analysis.

Qualitative results for the test, validation, and training sets are presented in
  \cref{fig:test_heatmaps_with_best_vali_model}.
These examples illustrate both success and failure cases.
The test and validation sets show clear responses to objects of interest, but the test set contains images
  of close-up and partially deteriorated poops.
This suggests a bias in the dataset towards ``fresh'' poops taken from some distance.

Notably, the much larger training set also contains errors, indicating more information can be extracted
  from this dataset using hard-negative mining.
There are clear difficult cases caused by sticks, leaves, pine cones, and dark areas on snow.
We note that while compiling these results, we checked over 1000 images and discovered 14 cases where an
  object failed to be annotated, and it is likely that more are missed, but we believe these cases are rare.

Although focal loss was used, the current learning curriculum is likely under-weighting smaller distant
  objects.
Our pixelwise evaluation metric is biased against this, which is a current limitation of our approach.
Future work evaluating this dataset on an object-detection level can remedy this.

\begin{figure*}[ht]
\centering
\includegraphics[width=1.0\textwidth]{figures/test_heatmaps_with_best_vali_model}%
\hfill
(a) Test set.
\includegraphics[width=1.0\textwidth]{figures/vali_heatmaps_with_best_vali_model.jpg}%
\hfill
(b) Validation set.
\includegraphics[width=1.0\textwidth]{figures/train_heatmaps_with_best_vali_model.jpg}%
\hfill
(c) Training set.
\caption[]{
    Qualitative results using the top-performing model on the validation set, applied to a selection of images
      from the (a) test, (b) validation, and (c) training sets.
    Success cases are presented on the left, with failure cases increasing towards the right.
    %
    Each figure is organized into three rows:
    %
    Top row:
    Binarized classification map, where true positive pixels are shown in white, false positives in red, false
      negatives in teal, and true negatives in black.
    The threshold for binarization was chosen to maximize the F1 score for each image, showcasing the best
      possible classification of the heatmap.
    Middle row:
    The predicted heatmap, illustrating the model's output before binarization.
    Bottom row:
    The input image, providing context for the prediction.
    %
    The majority of images in the test set exhibit qualitatively good results.
    Failure cases tend to occur with close-up images of older, sometimes partially deteriorated poops.
    These examples were manually selected and ordered to demonstrate dataset
    diversity in addition to representative results.
}
\label{fig:test_heatmaps_with_best_vali_model}
\end{figure*}


In \Cref{tab:parameters_and_results} we only presented the top results.
Here we've plotted the AP and AUC on the validation set for the top 5 AP-maximizing results from each of the
  7 training runs.
We also created a box-and-whisker plot for these top 5 results, which serves to assign a color and label to
  each training run.
These plots are shown in \Cref{fig:apauc_scatter}.


\begin{figure}[ht]
\centering
\begin{subfigure}[b]{0.4\textwidth}
 \includegraphics[width=\textwidth]{figures/macro_results_resolved_params.heatmap_pred_fit.trainer.default_root_dir_metrics.heatmap_eval.salient_AP_vs_metrics.heatmap_eval.salient_AUC_PLT02_scatter_nolegend.png}
 \caption{AP and AUC of 35 checkpoints.}
 \label{fig:apauc_scatter_a}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.4\textwidth}
 \includegraphics[width=\textwidth]{figures/macro_results_resolved_params.heatmap_pred_fit.trainer.default_root_dir_metrics.heatmap_eval.salient_AP_PLT04_box.png}
 \caption{AP of 35 checkpoints.}
 \label{fig:apauc_scatter_b}
\end{subfigure}
\caption{
    (a) Scatterplot of pixelwise average precision (AP) and Area Under the ROC curve (AUC) for the top
      5 checkpoints on the validation set.
    Points of the same color represent checkpoints from the same training run, which used identical
      hyperparameters.
    (b) Box-and-whisker plot the AP values across the top 5 checkpoints evaluated on
      the validation set.
    For each run, corresponding varied hyperparameters and maximum APs are given in
      \Cref{tab:parameters_and_results}.
}
\label{fig:apauc_scatter}
\end{figure}


\newcommand{\VitResourceCaption}{
\caption[]{
Resources used for training, prediction, and evaluation.
The "node" column is the pipeline stage:
"train" for training, "pred" for heatmap prediction, and "eval" for pixelwise heatmap evaluation.
The "resource" column lists the resource type: time, energy, or emissions.
The "total" and "\mu" columns show the total and average consumptions, and the "n" column indicates the
  frequency of each stage (e.g., across different hyperparameters).
Train rows marked with an asterisk (*) are based on indirect measurements.
}
\label{tab:resources}
}


\begin{table*}[ht]
\ifwacv \else \VitResourceCaption \fi
  \centering
  \begin{subtable}[b]{\textwidth} % Adjust width as needed
    \caption{Presented VIT experiment resources.}
    \centering
    \begin{tabular}{llllr}
    \toprule
            Node & Resource    &           Total  &           \mu &  n \\
    \midrule
    eval        &        time  & 14.24 hours      & 0.41 hours     &   35 \\
    \rule{0pt}{2ex}%
    pred        &        time  & 11.97 hours      & 0.34 hours     &   35 \\
    pred        &      energy  &  8.76 kWh        & 0.25 kWh       &   35 \\
    pred        &   emissions  &  1.84 \cotwo kg  & 0.05 \cotwo kg &   35 \\
    \rule{0pt}{2ex}%
    train$^{*}$ & time         &  39.22 days      & 5.60 days      &   7 \\
    train$^{*}$ & energy       & 324.75 kWh       & 46.39 kWh      &   7 \\
    train$^{*}$ & emissions    &  68.20 \cotwo kg & 9.74 \cotwo kg &   7 \\
    \bottomrule
    \end{tabular}
  \end{subtable}

  \hfill % Add horizontal space between the subfigures

  \begin{subtable}[b]{\textwidth} % Adjust width as needed
    \caption{All VIT experiment resources.}
    \centering
    \begin{tabular}{llllr}
    \toprule
            Node & Resource &           Total &            \mu &  n \\
    \midrule
    % Note: for presentation simplicity, we are rewriting the following row
    % so num agrees with other rGws. The reason the original value had an
    % additional number is because of rerun of one evaluation with different
    % parameters.
    % eval & time & 4 days 14:18:23 & 00:20:07 &  330 \\
    % 5.85 * 399/400 = 5.84
    %eval &        time &    5.85 day &  0.35 hours &  400 \\
    eval        &        time &    5.84 days     &  0.35 hours    &  399 \\
    \rule{0pt}{2ex}%
    pred        &        time &    7.29 days     &  0.44 hours    &  399 \\
    pred        &      energy &  102.83 kWh      &   0.26 kWh     &  399 \\
    pred        &   emissions &  21.6 \cotwo kg  & 0.05 \cotwo kg &  399 \\
    \rule{0pt}{2ex}%
    train$^{*}$ & time        & 158.95 days      &     3.78 days  &   42 \\
    train$^{*}$ & energy      & 1,316.07 kWh     &     31.34 kWh  &   42 \\
    train$^{*}$ & emissions   & 276.37 \cotwo kg & 6.58 \cotwo kg &   42 \\
    \bottomrule
    \end{tabular}
  \end{subtable}
\ifwacv \VitResourceCaption \fi
\end{table*}


\subsubsection{VIT Resource Usage}
\label{sec:vit_environmental_impact}

Note:
we remind the reader that this section only applies to the VIT models.

All models were trained on a single machine with an 11900k CPU and a 3090 GPU.
At predict time, using one background worker, our models processed 416 $\times$ 416 patches at a rate of
  20.93Hz with 94\% GPU utilization.

To better understand the energy requirements of our model, particularly for potential deployment on mobile
  devices, we used CodeCarbon \cite{lacoste2019codecarbon} to measure the resource usage during prediction and
  evaluation.
This analysis not only informs practical considerations but also helps us assess our contribution to the
  growing carbon footprint of AI \cite{kirkpatrick_carbon_2023}.
The results for the 7 presented training experiments and the total 42 training experiments are reported in
  \Cref{tab:resources}.

% See: ./scripts/estimate_training_resources.py
Direct measurement of resource usage during training is still under development, but we estimate the
  duration of each training run using indirect methods.
We approximate energy consumption by assuming a constant power draw of 345W from the 3090 GPU during
  training.
Emissions are estimated using a conversion ratio of 0.21 $\frac{\textrm{kg}\cotwo{}}{\textrm{kWh}}$.
  
Based on the validation set's 691 images, we estimate that predicting on a single image on our desktop
  requires approximately 1.15 seconds and 0.13 Wh of energy.
For context, typical mobile phones have a battery capacity of around 10 Wh and significantly less compute
  power than our desktop setup.
While our models demonstrate the feasibility of training a strong detector from our dataset, they are not
  optimized for the mobile setting.
To deploy our model on mobile devices, we will need to improve its efficiency or explore more efficient
  architectures.

\section{Environmental Impact} 
\label{sec:general_environmental_impact}

A footnote in the main paper reports the experiment costs, here we expand on the details.
These costs are the total over all runs in the development of this paper over different datasets, with
  different numbers of runs per model, so it cannot be used to infer running time of the models.
Instead it reports a component of the cost of performing this research.
All costs are estimated assuming \$0.16 per kWh, \$25 per 1000~kg CO$_2$.
The breakdown of resources used is given in \Cref{tab:resources_breakdown}.

Training accounted for the majority of resource usage with VIT models being the most expensive to run
  (\Cref{sec:vit_environmental_impact}).
The main reason is that VIT experiments operated on half-resolution images (2,016~$\times$~1,512) using
  416~$\times$~416 patches, whereas GroundingDINO, YOLO, and MaskRCNN were trained on smaller resized inputs
  (e.g., 640~$\times$~640, 1066~$\times$~800, depending on framework defaults) without windowing.
A second reason is that we trained many more VIT variants in a hyperparameter search, which was done before
  easy to use foundational models became available.
For training we estimated energy usage by measuring time and estimating GPU power draw approximated at 345W.
To estimate emissions we used a factor of 0.21~kg CO$_2$/kWh.

For prediction resources estimates, there is an important limitation.
The system running experiments was equipped with two RTX~3090 GPUs, although only a single one was used for
  any individual experiments.
However, due to our use of CodeCarbon, which counts entire system resources, some double counting may have
  occurred when we ran two experiments simultaneously.
Not all experiments were run in parallel, but some were.
Still our estimates provide an upper bound for the resources utilization and we the lower bound will at best
  be half of our reported numbers.
This limitation does not apply to our training estimations, which is the bulk of our cost, and thus our
  total numbers should only slightly inflated.

\newcommand{\TotalResourceCaption}{
\caption{Resource usage for training and prediction by model family.
Time is wall-clock duration on a single RTX~3090.
Energy is electricity consumed.
Emissions use a 0.21~\cotwo kg/kWh factor.
Cost is estimated at \$0.16/kWh electricity and \$25 per 1000~\cotwo kg.}
\label{tab:resources_breakdown}
}
\begin{table*}[hb]
\ifwacv \else \TotalResourceCaption \fi
\centering

\begin{tabular}{llrrrr}
\toprule
Phase & Model family   & Time (days) & Energy (kWh) & Emissions (\cotwo kg) & Cost (USD) \\
\midrule
Train
 & VIT-sseg      & 158.95 & 1316.07 & 276.37 & 217.48 \\
 & MaskRCNN      & 0.71   & 5.92    & 1.24   & 0.98 \\
 & YOLO-v9       & 4.14   & 34.30   & 7.20   & 5.67 \\
 & Grounding DINO& 0.32   & 2.68    & 0.56   & 0.44 \\
 & \textbf{Total (training)} & \textbf{164.12} & \textbf{1358.96} & \textbf{285.38} & \textbf{224.57} \\
\midrule
Test
 & VIT-sseg      & 13.13  & 102.83  & 21.60   & 16.99 \\
 & MaskRCNN      & 0.57   & 4.41    & 0.93    & 0.73 \\
 & YOLO-v9       & 0.08   & 0.19    & 0.02    & 0.03 \\
 & Grounding DINO& 0.13   & 0.29    & 0.02    & 0.05 \\
 & \textbf{Total (prediction)} & \textbf{13.91} & \textbf{107.72} & \textbf{22.57} & \textbf{17.80} \\
\midrule
\multicolumn{2}{l}{\textbf{Overall total}} & \textbf{178.03} & \textbf{1466.69} & \textbf{307.95} & \textbf{242.37} \\
\bottomrule
\end{tabular}
\ifwacv \TotalResourceCaption \fi
\end{table*}

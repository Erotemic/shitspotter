\documentclass{article}
%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
%\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts

% Define conditions
\newif\ifnonanonymous
\newif\ifuseappendix
\newif\ifuseacknowledgement


% ====================
% CONDITIONAL SETTINGS
% ====================
\nonanonymoustrue % comment out to be anonymous
\useacknowledgementtrue % comment out to remove acknowledgements
\useappendixtrue % comment out to remove appendix
% ====================

\ifnonanonymous
\usepackage[nonatbib,final]{neurips_2025}
% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2025}
\else
%\usepackage{neurips_2025}
\usepackage[nonatbib,preprint]{neurips_2025}
\fi

\newcommand{\redact}[1]{%
    \ifnonanonymous
        #1% Show the original text if nonanonymous is true
    \else
        [redacted for peer review]% Redact if false
    \fi
}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, 
% e.g. with the file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete *.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you should be clear).
%\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
%\usepackage[pagebackref,breaklinks,colorlinks,allcolors=cvprblue]{hyperref}
\usepackage{hyperref}


% Include other packages here, before hyperref.
%\usepackage{stfloats}

\usepackage{graphicx}
%\usepackage{emoji}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{framed}
\usepackage{booktabs}
\usepackage{comment}
\usepackage{multicol}
\usepackage{makecell}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage{cclicenses}


\usepackage{url}            % simple URL typesetting
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}         % Can be removed after putting your text content
\usepackage[numbers]{natbib}
\usepackage{doi}
\usepackage{seqsplit}


%% extra
\usepackage{listings}
\usepackage{amsmath} 
\usepackage{xcolor}
\usepackage[toc]{appendix}

%\usepackage{fontspec}
%\setmainfont{TeX Gyre Termes} % Or another font you like

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

\newcommand{\cotwo}{\ensuremath{\mathrm{CO_2}}}


\title{``ScatSpotter'' --- A Dog Poop Detection Dataset}

\author{Jonathan Crall\\
Kitware\\
\texttt{jon.crall@kitware.com} \\
%{\tt\small erotemic@gmail.com}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
%\and
%Second Author\\
%Institution2\\
%First line of institution2 address\\
%{\tt\small secondauthor@i2.org}
}

\begin{document}
\maketitle


%%%%%%%%% ABSTRACT
\begin{abstract}


We introduce a new dataset containing phone images of dog feces, annotated with manually drawn or AI-assisted polygon labels.  Its over 9000 ``before/after/negative'' full resolution images contain 6000 polygon annotations.  The collection and annotation of images started in late 2020.  This paper focuses on two checkpoints from 2025-04-20 and 2024-07-03.  We train VIT, MaskRCNN, YOLO-v9, and Grounding DINO baseline models to explore the difficulty of the dataset.  The best model achieves a box-level average precision of 0.69 on a 691-image validation set and 0.70 on a small independently captured 121-image contributor test set.  Dataset snapshots are available through four different distribution methods: two centralized (Girder and HuggingFace) and two decentralized (IPFS and BitTorrent).  We study of the trade-offs between distribution methods and discuss the feasibility of each with respect to reliably sharing open scientific data.  The code for experiments is hosted on GitHub.  The data license is CC-BY 4.0.  Model weights are available with the dataset.  Experiment hardware, time, energy, and emissions are quantified.

% Keywords: poop, feces, dataset, dataset distribution, detection, segmentation, IPFS, BitTorrent, HuggingFace

%We train a baseline vision transformer to segment the objects of interest, exploring a grid of hyperparameters, and we evaluate their impact. 
%A phone application to detect poop with these models is being developed and 
%will be made freely available.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}

Applications for a computer vision system capable of detecting and localizing poop in images are numerous.
These include automated waste disposal to keep parks and backyards clean, tools for monitoring wildlife
  populations via droppings, and a warning system in smart-glasses to prevent people from stepping in poop.
Our primary motivating use case is a phone application that assists dog owners in locating their dog's poop
  in a leafy park for easier cleanup.
Many of these applications can be realized with modern object detection and segmentation methods
  \cite{sandler_mobilenetv2_2018, siam_rtseg_2018, yu_mobilenet_yolo_2023} combined with a large labeled
  dataset. 
%In this paper, we make a significant step towards building this dataset.


\begin{figure}[t]
\centering
\begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/zoom_leaf.jpg}
    \caption[]{
        A zoomed in example of an annotated object in a challenging
        condition: a scene cluttered with leaves. The similarity between the leaves
        and the poop causes a camouflage effect that can make detecting it difficult.
        The poop is highlighted in blue.
    }
    \label{fig:HardCase}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/viz_three_images.jpg}
    \caption[]{
        The ``before/after/negative'' protocol.
        The orange box highlights the location of the poop 
        in the ``before'' image.
        In the ``after'' image, it is the same scene but the poop has been removed.
        The ``negative'' image is a nearby similar scene, potentially with a distractor.
        Note that the object is small relative to the image size.
    }
    \label{fig:ThreeImages}
\end{subfigure}
\caption{(a) A challenging annotation case due to camouflage. (b) The BAN protocol.}
\label{fig:Combined}
\end{figure}
  

\begin{table*}[t]
\caption{Related datasets.
%
Columns list dataset name, number of categories, images, and annotations.
Image W \times{} H gives median image dimensions;
Ann Area$^{0.5}$ is the median square root of annotation area (pixels);
Size is disk requirements in GB; 
Annot Type is the labeling method.
\Cref{fig:compare_allannots} shows the distribution of annotation shapes, sizes, and locations.
%Citations: ImageNet \cite{ILSVRC15}
%MSCOCO \cite{lin_microsoft_2014},
%CityScapes \cite{cordts2015cityscapes},
%ZeroWaste \cite{bashkirova_zerowaste_2022},
%TrashCanV1 \cite{hong2020trashcansemanticallysegmenteddatasetvisual},
%UAVVaste \cite{rs13050965},
%SpotGarbage-GINI \cite{mittal2016spotgarbage},
%TACO \cite{proenca_taco_2020},
%MSHIT \cite{mshit_2020}.
%Of the datasets in this table, ours has the highest image resolution.
%and the smallest annotation size relative to that resolution.
%Of the waste related datasets and in terms of number of images, ours is among the largest, and of the poop related datasets, it is the largest.
}
\label{tab:related_datasets}
\begin{tabular}{lrrrcrrl}
\toprule
Name & \#Cats & \#Images & \#Annots & \makecell{Image\\W \times{} H} & \makecell{Annot\\Area$^{0.5}$} & \makecell{Disk\\Size} & \makecell{Annot\\Type} \\
\midrule
ImageNet\cite{ILSVRC15}    & 1,000 & 594,546 & 695,776 & 500 \times{} 374 & 239 & 166GB & box \\
MSCOCO\cite{lin_microsoft_2014}      & 80 & 123,287 & 896,782 & 428 \times{} 640 & 57 & 50GB & polygon \\
CityScapes\cite{cordts2015cityscapes}  & 40 & 5,000 & 287,465 & 2,048 \times{} 1,024 & 50 & 78GB & polygon \\
ZeroWaste \cite{bashkirova_zerowaste_2022}   & 4 & 4,503 & 26,766 & 1,920 \times{} 1,080 & 200 & 10GB & polygon \\
TrashCanV1\cite{hong2020trashcansemanticallysegmenteddatasetvisual}  & 22 & 7,212 & 12,128 & 480 \times{} 270 & 54 & 0.61GB & polygon \\
UAVVaste\cite{rs13050965}    & 1 & 772 & 3,718 & 3,840 \times{} 2,160 & 55 & 2.9GB & polygon \\
SpotGarbage\cite{mittal2016spotgarbage} & 1 & 2,512 & 337 & 754 \times{} 754 & 355 & 1.5GB & category \\
TACO\cite{proenca_taco_2020}        & 60 & 1,500 & 4,784 & 2,448 \times{} 3,264 & 119 & 17GB & polygon \\
MSHIT\cite{mshit_2020}       & 2 & 769 & 2,348 & 960 \times{} 540 & 99 & 4GB & box \\
Ours        & 1 & 9,296 & 6,594 & 4,032 \times{} 3,024 & 87 & 60GB & polygon \\
\bottomrule
\end{tabular}
\end{table*}

\begin{figure*}[t]
\centering
\includegraphics[width=1.0\textwidth]{plots/appendix/dataset_compare/combo_all_polygons.png.png}
\caption[]{
    A comparison of all of the annotations for different datasets including ours.
    All polygon annotations drawn in a single plot with $0.8$ opacity to
    demonstrate the distribution in annotation location, shape, and size with
    respect to image coordinates.
}
\label{fig:compare_allannots}
\end{figure*}

\begin{figure*}[t]
\centering
\includegraphics[width=1\textwidth]{figures/umap-v3.jpg}%
\caption[]{
    %Example images from the dataset based on 2D UMAP \cite{mcinnes_umap_2020} clusters over the dataset.
    %Each point in the top image is a 2D-projected image embedding. Each
    %numbered orange dot corresponds to three nearby images, which are drawn in columns on the bottom.
    %Annotation boxes are drawn in blue.
    %An interesting observation is that there is a clear separation into two UMAP blobs represents snowy versus (columns 1 and 2)
    %  non-snowy images (columns 3-13). We verified that this pattern holds beyond the examples explicitly shown here.
    Example images from 2D UMAP clusters \cite{mcinnes_umap_2020}.
    Each point in the top image represents a 2D-projected embedding, with numbered orange dots indicating nearby
      images in the bottom columns.
    Blue annotation boxes are shown.
    A clear separation emerges between snowy (columns 1-2) and non-snowy images (columns 3-13).
    %this pattern verified beyond these examples.
    %a 200 image subset
    %  of the dataset.
    %Each row corresponds to a selection from a 2D UMAP projection shown on the left.
    %The highlighted nodes circled in blue in the cluster visualization in each row correspond to the images
    %  with annotations (drawn in green) shown on the right.
}
\label{fig:umap_dataset_viz}
\end{figure*}


In addition to enabling several applications, poop detection is an interesting benchmark problem.
It is relatively simple, with a narrow focus on a single class, making it suitable for exploring the
  capabilities of object detection models that target a single labeled class.
However, the task includes non-trivial challenges such as resolution issues (e.g., camera quality,
  distance), camouflaging distractors (e.g., leaves, pine cones, sticks, dirt, and mud), occlusion (e.g., bushes, overgrown
  grass), and variation in appearance (e.g., old vs. new, healthy vs. sick).
An example of a challenging case is shown in \Cref{fig:HardCase}.
Investigation into cases where this problem is difficult may provide insight
into how to better train object detection and segmentation networks.

Towards these ends we introduce a new dataset which, 
%for the purpose of this paper we call "ScatSpotter".
%we formally call ``ScatSpotter''.
in formal settings, we call ``ScatSpotter''.
Poops are annotated with polygons making the dataset suitable for training detection and segmentation
  models.
In order to assist with annotation and add variation, we collect images using a ``before/after/negative'' (BAN)
  protocol as shown in \Cref{fig:ThreeImages}.

From this data, we train a segmentation model to classify which pixels in an image contain poop and which do
  not.
Our models show strong performance, but there are notable failure cases indicating this problem is difficult
  even for modern computer vision algorithms. 

To enable others to build on our results, it is essential that the dataset is accessible and hosted
  reliably.
Centralized methods are a typical choice, offering high speeds, but they can be costly for individuals,
  often requiring institutional support or paid hosting services.
They are also prone to outages and lack built-in data validation.
In contrast, decentralized methods allow volunteers to host data and offers built-in validation of data
  integrity.
This motivates us to compare and contrast the decentralized BitTorrent \cite{cohen_incentives_2003}, and
  IPFS \cite{benet_ipfs_2014} protocols as mechanisms for distributing datasets.

% VGG2 face got removed.
% https://github.com/ox-vgg/vgg_face2/issues/52

Our contributions are:
1) A challenging new \textbf{open dataset} of images with polygon annotations.
2) A set of trained \textbf{baseline models}.
3) A \textbf{comparison of dataset distribution} methods.
%4) \textbf{Open code and models}.


%-------------------------------------------------------------------------
\section{Related Work}
\label{sec:relatedwork}

To the best of our knowledge, our dataset is currently the largest publicly available collection of
  annotated dog poop images, but it is not the first.
A dataset of 100 dog poop images was collected and used to train a FasterRCNN model
  \cite{neeraj_madan_dog_2019} but this dataset and model are not publicly available.
The company iRobot has a dataset of annotated indoor poop images used to train Roomba j7+ to avoid
  collisions \cite{roomba_2021}, but as far as we are aware, this is not available.
In terms of available poop detection datasets we are only aware of MSHIT~\cite{mshit_2020} which is much
  smaller, only contains box annotations, and the objects of interest are plastic toy poops.

Compared to benchmark object localization and segmentation datasets~\cite{ILSVRC15,
  lin_microsoft_2014,cordts2015cityscapes} ours is much smaller and focused only on a single category.
However, when compared to litter and trash datasets
  \cite{bashkirova_zerowaste_2022,proenca_taco_2020,hong2020trashcansemanticallysegmenteddatasetvisual,mittal2016spotgarbage,rs13050965}
  ours is among the largest in terms of number of images / annotations, image size, and total dataset size.
ZeroWaste~\cite{bashkirova_zerowaste_2022} uses a ``before/after'' protocol similar to our BAN protocol.
%% https://paperswithcode.com/dataset/tackknnno
We provide an overview of these related datasets in \Cref{tab:related_datasets}.
Among all of these, ours stands out for having the highest resolution images and the smallest objects
  relative to that resolution.
For a review of additional waste related datasets, refer to \cite{agnieszka_waste}.

\Cref{sec:distribution} discusses the logistics and tradeoffs between dataset distribution mechanisms
  with a focus on comparing centralized and decentralized methods.
IPFS~\cite{benet_ipfs_2014} and BitTorrent~\cite{cohen_incentives_2003} are the decentralized 
  mechanisms we evaluate, but others exist such as Secure Scuttlebut \cite{tarr_secure_2019} and Hypercore
  \cite{frazee_dep-0002_nodate}, which we did not test.

% Very good overview and comparison of the protocols
% https://blog.mauve.moe/posts/protocol-comparisons
% https://distributed.press/
% hypercore - https://github.com/tradle/why-hypercore/blob/master/FAQ.md#how-is-hypercore-different-from-ipfs
% git,
% Secure Scuttlebut (SSB)

\section{Dataset}
\label{sec:dataset}

Our first contribution is the creation of a new open dataset which consists of images of dog poop in mostly
  urban, mostly outdoor environments, from mostly a single city.
The data is annotated to support object detection and segmentation tasks.
The majority of the images feature fresh poop from three specific medium sized dogs, but there are
  a significant number of images with poops of unknown age and from unknown dogs.

Despite these biases, the dataset has significant image variations.
To provide a gist, we computed UMAP \cite{mcinnes_umap_2020} image embeddings based on ResNet50
  \cite{he2016deep} descriptors display images corresponding with clusters in this embedding in
  \Cref{fig:umap_dataset_viz}.

More details about the dataset are available in a standardized datasheet
\cite{gebru_datasheets_2021} that covers the motivation, composition,
collection, preprocessing, uses, distribution, and maintenance. This will be
distributed with the data itself, and is provided in supplemental material.

\subsection{Dataset Collection}

A single researcher on dog walks photographed fresh dog poop, mostly their own
dogs, but often others. Distance was sometimes varied for diversity. Most
images were taken following the ``before/after/negative'' (BAN) protocol.  
A BAN triple comprises a ``before'' shot of the poop, an ``after'' shot
post removal, and a ``negative'' shot of a nearby lookalike (e.g., pine cones,
leaves).  We only use them for negative sampling, but they could enable
contrastive triplet losses \cite{schroff_facenet_2015}.

The majority of images follow the BAN protocol, but there are exceptions.
The first six months of data collection only involved the ``before/after'' part of the protocol. 
We began collecting the third negative image after a colleague suggested it.
In some cases, the researcher failed or was unable to take the second or third image.
These exceptions are often programmatically identifiable.
  
We also received 121 contributor images, mostly outside the BAN protocol.
These images are held out and used as our test set.
%These are used only for testing and are \emph{excluded} from the analysis in \Cref{subsec:datastat}.
Due to the small size, our main results also include validation scores.
%The small size of this test set is the reason that our main results include
%validation and test scores.

\subsection{Dataset Annotation}

Images were annotated using labelme \cite{wada_labelmeailabelme_nodate}.
Most annotations were initialized using SAM and a point prompt.
All AI polygons were manually reviewed.
In most cases only small manual adjustments were needed, but there were a significant number of cases where
  SAM did not work well and fully manual annotations were needed.
Regions with shadows seemed to cause SAM the most trouble, but there were other failure cases.
Unfortunately, there is no metadata to indicate which polygons were manually created or done using AI.
However, the number of vertices may be a reasonable proxy to estimate this, as polygons generated by SAM
  tend to have higher fidelity boundaries.
The boundaries of the annotated polygons are illustrated in \Cref{fig:compare_allannots}.

Data collected after 2024-07-03 was annotated with the help of models trained
on prior data. Again, all predictions were manually verified or corrected. In
these later cases, false positive annotations were labeled (e.g. stick, leaf),
but because these categories are not labeled exhaustively, we exclude them from
all analysis in this paper.


\begin{figure}[t]
\centering
\begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/images_timeofday_distribution.png}
    \caption{
        The time-of-year vs time-of-day of each image show lighting and seasonal
        variation.  On the x-axis, 0 is January 1st. On the y-axis, 0 is
        midnight.  Color estimates daylight based on location (if available).
        Most images are in the day, but many were taken at night with flash or
        long exposure.
    }
    \label{fig:TimeOfDayDistribution}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/anns_per_image_histogram_splity.png}
    \caption{
        The histogram of annotations per image shows object density variation.
        Only 35\% (3,314) of images contain annotations; 65\% (5,982) are known negatives.
        About half of the negatives were taken immediately after pickup; the
        rest are from nearby locations with potential lookalikes.
    }
    \label{fig:AnnotsPerImage}
\end{subfigure}
\caption{Dataset distributions. (a) Time and daylight scatterplot. (b) Annotation count histogram.}
\label{fig:TimeAndAnnots}
\end{figure}



\subsection{Dataset Properties and Statistics}
\label{subsec:datastat}

% Number of images, annotations, and other stats.

%import kwutil
%kwutil.datetime.coerce('now') - kwutil.datetime.coerce('2020-12-18')
%kwutil.datetime.coerce('2025-04-20') - kwutil.datetime.coerce('2020-12-18')

The data was captured at a regular rate over 4.3 years, primarily in parks and sidewalks within a small
  city.
Weather conditions varied across snowy, sunny, rainy, and foggy.
A visual representation of the distribution of seasons, time-of-day, daylight, and capture rate is provided
  in \Cref{fig:TimeOfDayDistribution}.

The dataset images are available in full resolution.
Almost all images were taken using the same phone-camera, with a consistent width/height of 4,032
  $\times$ 3,024 (although some may be rotated based on EXIF data).
The images are stored as 8-bit JPEGs with RGB channels, and most include overviews (i.e., image pyramids),
  allowing for fast loading of downscaled versions.
%Six images have a slightly different resolution of 4,008 $\times$ 5,344, and one has a resolution of 7,680
%  $\times$ 1,024.


Due to the BAN protocol, about one-third of the images contain
annotations, the rest were taken after the object(s) were removed.  Consequently, most
images have no annotations. When present, annotations are usually singular, but
multiple annotations are common and can be due to:
1) fragmented dropping,
2) dogs pooping together,
3) repeated poops in the same area over time (sometimes hard to distinguish from dirt).
The number of annotations per image is illustrated in \Cref{fig:AnnotsPerImage}.


\subsection{Dataset Splits}

Our dataset is split into training, validation, and test sets based on the year and day of image capture and
  photographer.
Only data captured by the authors is used for training and validation.
Of these, images from 2021-2023, 2025 and beyond are assigned to the training set. 
Images from 2020 are used for
  validation.
For data from 2024, we consider the ordinal date $n$ of each image and include it in the validation set if
  $n \equiv 0 \ (\textrm{mod}\ 3)$; otherwise, it is assigned to the training set.


For testing data, we use contributor images to not bias our results based on the way the authors took
  images.
These splits are provided in the COCO JSON format \cite{lin_microsoft_2014} as well as a WebDataset
  \cite{huggingfacewebdataset} on HuggingFace.

\section{Baseline Models}
\label{sec:models}

TODO: introduce and discuss updated model results \cite{wang2024yolov9,liu_grounding_2024,OpenGroundingDino}.

As our second contribution, we trained and evaluated models to establish a baseline for future comparisons.
Specifically we train 7 model variants.
We trained a semantic segmentation vision transformer variant (VIT-sseg-s)
  \cite{Greenwell_2024_WACV,crall_geowatch_2024}, which was only trained from scratch.
We trained two MaskRCNN \cite{he2017mask} models (specifically the \texttt{R\_50\_FPN\_3x} configuration),
  one starting from pretrained ImageNet weights (MaskRCNN-p), and one starting from scratch
  (MaskRCNN-s).
Similarly we trained YOLO-v9 \cite{wang2024yolov9} both from scratch and using pretrained ImageNet weights.
Lastly, we investigated evaluated the foundational Grounding DINO \cite{liu_grounding_2024} model. 
In the zero-shot setting we used \texttt{IDEA-Research/grounding-dino-tiny} using the prompt: "animalfeces".
Finally, we tune the same Grounding DINO using \cite{OpenGroundingDino} and evaluated it in the tuned setting.


Hyperparameters are given in supplemental materials.
\Cref{sec:model_hyperparams}


For these baseline models, the training data was limited to an older subset taken before 2024-07-03.
Our training dataset consists of 5,747 images and is identified by a suffix of {\tt 1e73d54f}, which is the
  prefix of its content hash.
The validation set contains 691 images and has a suffix of {\tt 99b22ad0}.
The test set, consists of the 121 images, has a suffix of {\tt 6cb3b6ff}, and includes contributor images
  up to 2025-04-20.
The evaluated models were selected based on their Box-AP validation scores.

We performed two types of evaluations on the models.
``Box'' evaluation computes standard COCO object detection metrics \cite{lin_microsoft_2014}.
MaskRCNN natively outputs scored bounding boxes, but for the VIT-sseg model, we convert heatmaps into boxes
  by thresholding the probability maps and converting taking the extend of the resulting polygons as bounding
  boxes.
The score is taken as the average heatmap response under the polygon.
Bounding box evaluation has the advantage that small and large annotations contribute equally to the score,
  but it can also be misleading for datasets where the notion of an object instance can be ambiguous.

To complement the box evaluation, we performed a pixelwise evaluation, which is more sensitive to the
  details of the segmented masks, but also can be biased towards larger annotations with more pixels.
The corresponding truth and predicted pixels were accumulated into a confusion matrix, allowing us to
  compute standard metrics \cite{powers_evaluation_2011} such as precision, recall, false positive rate, etc.
For the VIT-sseg model, computing this score is straightforward, but for MaskRCNN we accumulate per-box
  heatmaps into a larger full image heatmap, which can then be scored.
YOLO-v9 and Grounding DINO do not produce masks, thus we omit pixelwise for these.

Quantitative results for each of these models on box and pixel metrics are shown in
  \Cref{tab:model_results}.
Because the independent test set is only 121 images, we also present results on the larger validation
  dataset.
Corresponding qualitative test results are illustrated in \Cref{fig:test_results_all_models} and validation
  results in \Cref{fig:vali_results_all_models}.

% Table generation
% ~/code/shitspotter/papers/neurips-2025/scripts/build_v2_result_table.py

\newcommand{\tb}[1]{\textbf{#1}}

\begin{table}[t]
\caption[]{
    Results for MaskRCNN and VIT models (suffix -p: pretrained, -s: scratch on test and validation sets.
    Evaluated with box and pixel metrics --- AP (ppv-tpr area) \cite{powers_evaluation_2011} and AUC (tpr-fpr area), F1, and TPR (recall) --- computed via scikit-learn \cite{scikit-learn}.
    Pretrained models outperform.
    Note: VIT-sseg was tuned more operated at full resolution; MaskRCNN, DINO, and YOLO used resized images and may yield better results with similar effort.
    %Results of MaskRCNN and VIT models. A suffix -p is pretrained, and -s is from scratch.
    %Quantitative results on the test and validation datasets.
    %Unsurprisingly, the model starting with pretrained weights scores best.
    %Models are evaluated using bounding-box metrics (under the Box column) as well as pixelwise-segmentation
    %  metrics (under the Pixel column).
    %We consider positive predictive value (ppv or precision), true-positive-rate (tpr or recall), and false positive rate (fpr).
    %The average precision (AP) is the area under the ppv/tpr curve \cite{powers_evaluation_2011}.
    %The AUC is the area under the tpr/fpr curve.
    %Thus AP is more sensitive to ppv and AUC is more sensitive to fpr.
    %All metrics were computed using scikit-learn \cite{scikit-learn}.
    %We note an important limitation of our results:
    %much more time was spent tuning the VIT-sseg model.
    %It is likely that MaskRCNN results could be improved with further tuning.
    %But these are baseline models; our core contribution is the dataset.
}
\label{tab:model_results}
\centering

(a) Validation (n=691)

\begin{tabular}{lllllllll}
\hline
 model           & \makecell{AP\\Box}   & \makecell{AUC\\Box}   & \makecell{F1\\Box}   & \makecell{TPR\\Box}   & \makecell{AP\\Pixel}   & \makecell{AUC\\Pixel}   & \makecell{F1\\Pixel}   & \makecell{TPR\\Pixel}   \\
\hline
 MaskRCNN-p      & 0.61                 & \textbf{0.72}         & 0.62                 & 0.57                  & 0.74                   & 0.91                    & \textbf{0.74}          & 0.68                    \\
 MaskRCNN-s      & 0.26                 & 0.58                  & 0.35                 & 0.31                  & 0.43                   & 0.89                    & 0.48                   & 0.50                    \\
 VIT-sseg-s      & 0.48                 & 0.53                  & 0.60                 & 0.51                  & \textbf{0.76}          & \textbf{0.97}           & 0.74                   & \textbf{0.69}           \\
 GroundingDINO-t & \textbf{0.69}        & 0.63                  & \textbf{0.74}        & \textbf{0.68}         & --                     & --                      & --                     & --                      \\
 GroundingDINO-z & 0.08                 & 0.21                  & 0.20                 & 0.25                  & --                     & --                      & --                     & --                      \\
 YOLO-v9-p       & 0.41                 & 0.59                  & 0.50                 & 0.42                  & --                     & --                      & --                     & --                      \\
 YOLO-v9-s       & 0.33                 & 0.41                  & 0.44                 & 0.37                  & --                     & --                      & --                     & --                      \\
\hline
\end{tabular}


(b) Test (n=121)

\begin{tabular}{lllllllll}
\hline
 model           & \makecell{AP\\Box}   & \makecell{AUC\\Box}   & \makecell{F1\\Box}   & \makecell{TPR\\Box}   & \makecell{AP\\Pixel}   & \makecell{AUC\\Pixel}   & \makecell{F1\\Pixel}   & \makecell{TPR\\Pixel}     \\
\hline
 MaskRCNN-p      & 0.61          & \textbf{0.70} & 0.65          & 0.60          & \textbf{0.81} & \textbf{0.85} & \textbf{0.78} & \textbf{0.73} \\
 MaskRCNN-s      & 0.25          & 0.47          & 0.34          & 0.30          & 0.39          & 0.80          & 0.41          & 0.44          \\
 VIT-sseg-s      & 0.39          & 0.40          & 0.52          & 0.41          & 0.41          & 0.82          & 0.48          & 0.37          \\
 GroundingDINO-t & \textbf{0.70} & 0.67          & \textbf{0.76} & \textbf{0.68} & --            & --            & --            & --            \\
 GroundingDINO-z & 0.23          & 0.30          & 0.39          & 0.38          & --            & --            & --            & --            \\
 YOLO-v9-p       & 0.44          & 0.55          & 0.51          & 0.50          & --            & --            & --            & --            \\
 YOLO-v9-s       & 0.36          & 0.36          & 0.48          & 0.37          & --            & --            & --            & --            \\
\hline

\end{tabular}



%\begin{tabular}{ll rrrr rrrr}
%\toprule
%\multicolumn{2}{c}{Dataset split:} & \multicolumn{4}{c}{Test (n=121)} & \multicolumn{4}{c}{Validation (n=691)} \\
%%\multicolumn{2}{c}{Evaluation type:} & \multicolumn{2}{c}{Box} & \multicolumn{2}{c}{Pixel} & \multicolumn{2}{c}{Box} & \multicolumn{2}{c}{Pixel} \\
%\multicolumn{2}{c}{Evaluation type:} & Box & Box & Pixel & Pixel & Box & Box & Pixel & Pixel \\
%%%%                      T-Box        T-Box        T-Pixel      T-Pixel    | V-Box        V-Box        V-Pixel      V-Pixel
%Model type & \# Params & AP         & AUC        & AP         & AUC        & AP         & AUC        & AP         & AUC \\
%\midrule
%MaskRCNN-p & 43.9e6    & \tb{0.613} & \tb{0.697} & \tb{0.810} & 0.849      & \tb{0.612} & \tb{0.721} & \tb{0.858} & 0.905 \\
%MaskRCNN-s & 43.9e6    & 0.253      & 0.464      & 0.384      & 0.798      & 0.255      & 0.576      & 0.434      & 0.891 \\
%VIT-s      & 25.5e6    & 0.422      & 0.426      & 0.473      & \tb{0.902} & 0.476      & 0.532      & 0.780      & \tb{0.994} \\
%GroundingDINO-p & --    & --      & --       & --        & --    & --        & --       & --       & --    \\
%GroundingDINO-z & --    & --      & --       & --        & --    & --        & --       & --       & --    \\
%YOLO-p          & --    & --      & --       & --        & --    & --        & --       & --       & --    \\
%YOLO-s          & --    & --      & --       & --        & --    & --        & --       & --       & --    \\
%\bottomrule
%\end{tabular}

%\begin{tabular}{ll rrr rrr rrr rrr}
%\toprule
%\multicolumn{2}{c}{Dataset split:} & \multicolumn{6}{c}{Test (n=121)} & \multicolumn{6}{c}{Validation (n=691)} \\
%\multicolumn{2}{c}{Evaluation type:} & AP & AUC & F1 & AP & AUC & F1 & AP & AUC & F1 & AP & AUC & F1 \\
% & & \multicolumn{3}{c}{Box} & \multicolumn{3}{c}{Pixel} & \multicolumn{3}{c}{Box} & \multicolumn{3}{c}{Pixel} \\
%\midrule
%MaskRCNN-p & 43.9e6 & 0.613 & \tb{0.698} & 0.650 & \tb{0.871} & 0.816 & \tb{0.885} & 0.612 & \tb{0.721} & 0.625 & \tb{0.858} & 0.883 & \tb{0.839} \\
%MaskRCNN-s & 43.9e6 & 0.254 & 0.465 & 0.345 & 0.385 & 0.798 & 0.408 & 0.255 & 0.576 & 0.349 & 0.434 & 0.891 & 0.482 \\
%VIT-s & 25.5e6 & 0.393 & 0.404 & 0.517 & 0.473 & \tb{0.902} & 0.520 & 0.476 & 0.532 & 0.596 & 0.780 & \tb{0.994} & 0.746 \\
%GroundingDINO-p & 172.2e6 & \tb{0.700} & 0.666 & \tb{0.758} & -- & -- & -- & \tb{0.691} & 0.631 & \tb{0.743} & -- & -- & -- \\
%GroundingDINO-z & 172.2e6 & 0.228 & 0.303 & 0.393 & -- & -- & -- & 0.078 & 0.210 & 0.197 & -- & -- & -- \\
%YOLO-p & 51.0e6 & 0.441 & 0.551 & 0.507 & -- & -- & -- & 0.411 & 0.595 & 0.496 & -- & -- & -- \\
%YOLO-s & 51.0e6 & 0.377 & 0.408 & 0.469 & -- & -- & -- & 0.264 & 0.427 & 0.381 & -- & -- & -- \\
%\bottomrule
%\end{tabular}

\end{table}


%\begin{figure*}[t]
%\centering
%\includegraphics[width=1.0\textwidth]{figures/agg_viz_results/test_imgs30_d8988f8c.kwcoco/results_detectron-pretrained.jpg}%
%\hfill
%(a) MaskRCNN-pretrained (test set results).
%\includegraphics[width=1.0\textwidth]{figures/agg_viz_results/test_imgs30_d8988f8c.kwcoco/results_detectron-scratch.jpg}%
%\hfill
%(b) MaskRCNN-scratch (test set results).
%\includegraphics[width=1.0\textwidth]{figures/agg_viz_results/test_imgs30_d8988f8c.kwcoco/results_geowatch-scratch.jpg}%
%\hfill
%(c) VIT-sseg-scratch (test set results).
%\includegraphics[width=1.0\textwidth]{figures/agg_viz_results/test_imgs30_d8988f8c.kwcoco/results_input_images.jpg}%
%\hfill
%(d) Input images from the test set.
%\caption[]{

%    Qualitative results from the top model on the validation set, applied to test images.
%    The first three subfigures (a, b, c) display a binarized classification map (true positives in green, false
%      positives in red, false negatives in purple, true negatives in black) and the predicted heatmap (before
%      binarization).

%    %The first three subfigures (a, b, c) display a binarized classification map (true positives in white, false
%    %  positives in red, false negatives in teal, true negatives in black) and the predicted heatmap (before
%    %  binarization).
%    Subfigure (d) shows the input image.
%    The heatmap binarization threshold was 0.5.
%    Failures occur with close-up or deteriorated objects, and camouflage.

%    %Qualitative results using the top-performing model on the validation set, applied to a selection of
%    %  images from the test set.
%    %Subfigure (d) shows the input image for the above predictions.
%    %In the first three subfigures (a, b, and c), the top row is a binarized classification map, where true
%    %  positive pixels are shown in white, false positives in red, false negatives in teal, and true negatives
%    %  in black.
%    %The second row in each subfigure is the predicted heatmap, illustrating the model's output before
%    %  binarization.
%    %The threshold for binarization was set to $0.5$ in all cases.
%    %All three methods show clear responses to objects of interest, but cases where objects are close-up 
%    %  and partially deteriorated do seem to be a common failure mode.
%    %Camouflage is likely a failure case, but this dataset does not contain
%    %  many examples.
    
%}
%\label{fig:test_results_all_models}
%\end{figure*}


%\begin{figure*}[t]
%\centering
%\includegraphics[width=1.0\textwidth]{figures/agg_viz_results/vali_imgs691_99b22ad0.kwcoco/results_detectron-pretrained.jpg}%
%\hfill
%(a) MaskRCNN-pretrained (validation set results).
%\includegraphics[width=1.0\textwidth]{figures/agg_viz_results/vali_imgs691_99b22ad0.kwcoco/results_detectron-scratch.jpg}%
%\hfill
%(b) MaskRCNN-scratch (validation set results).
%\includegraphics[width=1.0\textwidth]{figures/agg_viz_results/vali_imgs691_99b22ad0.kwcoco/results_geowatch-scratch.jpg}%
%\hfill
%(c) VIT-sseg-scratch (validation set results).
%\includegraphics[width=1.0\textwidth]{figures/agg_viz_results/vali_imgs691_99b22ad0.kwcoco/results_input_images.jpg}%
%\hfill
%(d) Inputs from the validation set.
%\caption[]{
%    %Qualitative results using the top-performing model on the validation set, applied to a selection of
%    %  images from the validation set. See \Cref{fig:test_results_all_models} for an explanation of the visualizations.
%    %Each model was selected based on its performance on this dataset, which may
%    %  cause spurious cases that agree with the truth labels, but this dataset
%    %  was never used to compute a gradient, which still make these valuable
%    %  results for assessing generalizability. Notably the models were able to
%    %  pick out camouflaged cases on the left, but not all on the right.
%Qualitative results of the top model on unseen validation images (see \Cref{fig:test_results_all_models} for visualization details). Although never trained on these data, the model's was able to detect camouflaged cases on the left but missed some on the right, indicating generalizability but also room for improvement.}
%\label{fig:vali_results_all_models}
%\end{figure*}

\begin{figure*}[ht]
\centering
\includegraphics[width=1.0\textwidth]{figures/agg_viz_results2/test_imgs121_6cb3b6ff.kwcoco/results_geowatch-scratch_heatmap_confusion_components.jpg}%
\hfill
(a) VIT-sseg-scratch (Test set results)
\includegraphics[width=1.0\textwidth]{figures/agg_viz_results2/test_imgs121_6cb3b6ff.kwcoco/results_detectron-pretrained_heatmap_confusion_components.jpg}%
\hfill
(b) MaskRCNN-pretrained (Test set results)
\includegraphics[width=1.0\textwidth]{figures/agg_viz_results2/test_imgs121_6cb3b6ff.kwcoco/results_detectron-scratch_heatmap_confusion_components.jpg}%
\hfill
(c) MaskRCNN-scratch (Test set results)
\includegraphics[width=1.0\textwidth]{figures/agg_viz_results2/test_imgs121_6cb3b6ff.kwcoco/results_yolo_v9-scratch_detection_confusion.jpg}%
\hfill
(d) YOLO-v9-scratch (Test set results)
\includegraphics[width=1.0\textwidth]{figures/agg_viz_results2/test_imgs121_6cb3b6ff.kwcoco/results_yolo_v9-pretrained_detection_confusion.jpg}%
\hfill
(e) YOLO-v9-pretrained (Test set results)
\includegraphics[width=1.0\textwidth]{figures/agg_viz_results2/test_imgs121_6cb3b6ff.kwcoco/results_grounding_dino-zero_detection_confusion.jpg}%
\hfill
(f) GroundingDINO-zero (Test set results)
\includegraphics[width=1.0\textwidth]{figures/agg_viz_results2/test_imgs121_6cb3b6ff.kwcoco/results_grounding_dino-tuned_detection_confusion.jpg}%
\hfill
(g) GroundingDINO-tuned (Test set results)
\includegraphics[width=1.0\textwidth]{figures/agg_viz_results2/test_imgs121_6cb3b6ff.kwcoco/results_input_images.jpg}%
\hfill
(h) Inputs from the Test set
\caption[]{
    Qualitative results using the top-performing models on the validation set.
}
\label{fig:test_results_all_models}
\end{figure*}


\begin{figure*}[ht]
\centering
\includegraphics[width=1.0\textwidth]{figures/agg_viz_results2/vali_imgs691_99b22ad0.kwcoco/results_geowatch-scratch_heatmap_confusion_components.jpg}%
\hfill
(a) VIT-sseg-scratch (validation set results)
\includegraphics[width=1.0\textwidth]{figures/agg_viz_results2/vali_imgs691_99b22ad0.kwcoco/results_detectron-pretrained_heatmap_confusion_components.jpg}%
\hfill
(b) MaskRCNN-pretrained (validation set results)
\includegraphics[width=1.0\textwidth]{figures/agg_viz_results2/vali_imgs691_99b22ad0.kwcoco/results_detectron-scratch_heatmap_confusion_components.jpg}%
\hfill
(c) MaskRCNN-scratch (validation set results)
\includegraphics[width=1.0\textwidth]{figures/agg_viz_results2/vali_imgs691_99b22ad0.kwcoco/results_yolo_v9-scratch_detection_confusion.jpg}%
\hfill
(d) YOLO-v9-scratch (validation set results)
\includegraphics[width=1.0\textwidth]{figures/agg_viz_results2/vali_imgs691_99b22ad0.kwcoco/results_yolo_v9-pretrained_detection_confusion.jpg}%
\hfill
(e) YOLO-v9-pretrained (validation set results)
\includegraphics[width=1.0\textwidth]{figures/agg_viz_results2/vali_imgs691_99b22ad0.kwcoco/results_grounding_dino-zero_detection_confusion.jpg}%
\hfill
(f) GroundingDINO-zero (validation set results)
\includegraphics[width=1.0\textwidth]{figures/agg_viz_results2/vali_imgs691_99b22ad0.kwcoco/results_grounding_dino-tuned_detection_confusion.jpg}%
\hfill
(g) GroundingDINO-tuned (validation set results)
\includegraphics[width=1.0\textwidth]{figures/agg_viz_results2/vali_imgs691_99b22ad0.kwcoco/results_input_images.jpg}%
\hfill
(h) Inputs from the validation set
\caption[]{
    Qualitative results using the top-performing models on the validation set.
}
\label{fig:vali_results_all_models}
\end{figure*}


All models were trained on a single machine with an Intel Core i9-11900K CPU and an NVIDIA GeForce RTX 3090
GPU. 
Our environmental impact
\footnote{For the most demanding experiments (VIT and MaskRCNN), prediction and evaluation took 15.6 days, consuming 109.63 kWh and emitting 23.0~\cotwo~kg (CodeCarbon \cite{lacoste2019codecarbon}). Training was estimated at 159.66 days and 1321.99 kWh, yielding 277.61~\cotwo~kg, assuming a 345W GPU draw and a 0.21~$\frac{\textrm{kg}\cotwo{}}{\textrm{kWh}}$ emission factor. At $\$0.16$/kWh and $\$25$/tonne~\cotwo, total cost was \$229.06. More details in in \Cref{environmental_impact}.}
was minimal.
See more on details in \Cref{sec:experiment_details}.

%\footnote{The total time spent on prediction and evaluation across all experiments was 15.6 days, with prediction consuming 109.63 kWh of energy and causing an estimated emissions of 23.0 \cotwo kg as measured by CodeCarbon \cite{lacoste2019codecarbon}. We estimated train-time resource usage using indirect methods, assuming a constant power draw of 345W from the RTX 3090 GPU. Energy consumption was approximated accordingly, and emissions were calculated using a conversion ratio of 0.21 $\frac{\textrm{kg}\cotwo{}}{\textrm{kWh}}$ derived from our prediction-time measurements. Based on file timestamps, we estimated that running 44 different training runs took approximately 159.66 days, resulting in an estimated energy usage and emissions of 1321.99 kWh and 277.612 $\cotwo$ kg, respectively. At $\frac{\$0.16}{\textrm{kWh}}$ and $\frac{\$25.00}{1000 \cotwo \textrm{kg}}$, the cost of training and evaluation was approximately \$229.06.}



\subsection{Dataset Transfer Experiment}

Our third contribution is an experiment that studies transfer rates of decentralized and centralized data
  distribution methods.
For centralized distribution, we use a self-hosted instance of Girder~\cite{girder_2024} and the HuggingFace
  datasets~\cite{huggingface_datasets} platform.
For decentralized clients, we use Transmission~\cite{transmission_2024} (BitTorrent) and
  Kubo~\cite{ipfskubo_2024} (IPFS).
As a baseline, we also measure direct transfers using Rsync~\cite{rsyncprojectrsync_2024}.

For data transfer experiments, we use the 2024-07-03 version of the dataset. 
This is content-addressed with the IPFS CID (content identifier):
\texttt{\seqsplit{bafybeiedwp2zvmdyb2c2axrcl455xfbv2mgdbhgkc3dile4dftiimwth2y}}.
%{\tt bafybei edwp2zvmdyb2c 2axrcl455xfbv 2mgdbhgkc3dil e4dftiimwth2y}.
%{\tt bafybeiedwp2zvmdyb2c2axrcl455xfbv2mgdbhgkc3dile4dftiimwth2y}.
%\begin{lstlisting}[basicstyle=\normalsize]
%bafybeiedwp2zvmdyb2c2axrcl455x
%fbv2mgdbhgkc3dile4dftiimwth2y
%\end{lstlisting}
The torrent has a magnet URL of:
\texttt{\seqsplit{magnet:?xt=urn:btih:ee8d2c87a39ea9bfe48bef7eb4ca12eb68852c49}},
%{\tt magnet:?xt=urn:btih:ee8d2c87a39ea9bfe48bef7eb4ca12eb68852c49},
and is tracked on Academic Torrents \cite{academic_torrents_Cohen2014}.
%\begin{lstlisting}[basicstyle=\normalsize]
%\end{lstlisting}
More details are in \Cref{sec:datset_discuss}.


%%\begin{wraptable}{r}{0.5\textwidth}
%%\small
%\begin{table}[t]
%%\vspace{-1.2em} % optional tweak
%\caption[]{
%Transfer times (in hours) for our 42GB dataset: trials (n), mean (\mu), std (\sigma).
%Each experiment was run 5 times.
%Suffix (-u) means uncompressed, (-c) means compressed.
%Uncompressed transfers provide granular access to individual files, but compressed transfers are faster.
%}
%\label{tab:transfertime}
%\centering
%\setlength{\tabcolsep}{5.35pt} % Reduce horizontal padding
%\begin{tabular}{lrrrr}
%\toprule
%{}           &        \mu &     \sigma &   Min &    Max \\
%Method       &            &            &       &        \\
%\midrule       
%BitTorrent-u &      8.36h &      5.16h & 2.21h & 14.39h \\
%IPFS-u       &     10.68h &      9.54h & 1.80h & 24.62h \\
%Rsync-u      &      4.84h &      1.39h & 3.10h &  6.10h \\
%Girder-c     &      2.85h &      2.31h & 1.05h &  6.24h \\
%HuggingFace-c & \bf{0.14h} &      0.03h & 0.11h &  0.18h \\
%Rsync-c      &      1.10h &      0.03h & 1.07h &  1.13h \\
%\bottomrule
%\end{tabular}
%%\end{wraptable}
%\end{table}


\begin{table}[t]
\caption{
Transfer times (in hours) for our 42GB dataset: trials (n), mean ($\mu$), std ($\sigma$).
Each experiment was run 5 times.
Uncompressed transfers provide granular access to individual files, while compressed transfers are faster.
}
\label{tab:transfertime}
\centering
\setlength{\tabcolsep}{4.5pt} % Adjusted padding for new column
\begin{tabular}{lcrrrr}
\toprule
Method       & Compressed & $\mu$     & $\sigma$ & Min    & Max     \\
\midrule       
BitTorrent   & No         & 8.36h     & 5.16h    & 2.21h  & 14.39h  \\
IPFS         & No         & 10.68h    & 9.54h    & 1.80h  & 24.62h  \\
Rsync        & No         & 4.84h     & 1.39h    & 3.10h  & 6.10h   \\
Girder       & Yes        & 2.85h     & 2.31h    & 1.05h  & 6.24h   \\
HuggingFace  & Yes        & \bf{0.14h}& 0.03h    & 0.11h  & 0.18h   \\
Rsync        & Yes        & 1.10h     & 0.03h    & 1.07h  & 1.13h   \\
\bottomrule
\end{tabular}
\end{table}

% https://huggingface.co/papers/2307.12169
% https://github.com/huggingface/hf_transfer
% https://github.com/huggingface/datasets
% https://arxiv.org/pdf/1804.07617


The HuggingFace results stand out, as they are faster than rsync.
We believe this is due to an optimized client and content delivery networks, utilizing CAKE
  \cite{hoiland2018piece} to minimize buffer bloat \cite{gettys2012bufferbloat}.
However, this speed relies on costly centralized infrastructure.
The expected speed from a more modest centralized service is $\sim\!20\times$ slower.

There is an additional $\sim\!4\times$  slowdown between compressed and uncompressed rsync baselines, which needs to be
  considered when comparing decentralized results.
The minimum time column shows that decentralized methods method can be competitive with rsync, but on
  average decentralized mechanisms are significantly slower and can be stifled by long peer-discovery times.
  
\section{Conclusion}

We have introduced the largest open dataset of high resolution images with polygon
  segmentations of dog poop.
The dataset contains several challenges including amorphous objects, multi-season variation, difficult
  distractors, daytime / nighttime variation.
We have described the dataset collection and annotation process and reported statistics on the dataset.

We provided a recommended train/validation/test split of the dataset, and trained baseline segmentation
  models that perform well, but could likely be improved.
In addition to providing quantitative and qualitative results of the models, we also estimate the resources
  required to perform these training, prediction, and evaluation experiments.

We have published our data and models under a permissive license, and made them available through both
  centralized (Girder and HuggingFace) and decentralized (BitTorrent and IPFS) mechanisms.
Decentralized methods have robustness properties, but suffer from significant network transfer overhead.
HuggingFace has exceptionally fast transfer speeds, and due to its usage of git-lfs has some decentralized
  properties, but lacks content identifiers.
Combining IPFS with a content distribution network may be a path to a best-of-both-worlds system.
%It may be possible to build a best of both worlds protocol and distribution network.


Limitations of our work include:
1) geographic concentration of the dataset,
2) the small size of the independent test set,
3) limited exploration of the better-performing model variant, and
4) uncontrolled network conditions during distribution experiments.
Future work could address these by expanding dataset diversity, training a
broader range of models, and improving decentralized hosting strategies.

Our dataset enables applications such as mobile apps for detecting feces, urban
cleanliness monitoring, and augmented reality collision warnings. We believe
negative impacts are limited and expect respectful use of the dataset.
We envision exciting possibilities for the BAN protocol in computer vision research.
We hope our work will inspire others to consider decentralized content addressable data sharing, fostering
  open collaboration and reproducible experiments.
Furthermore, we encourage the community to track experimental resource usage to better understand and offset
  our experiments' small, but real environmental impact.
Moreover, we aspire for our dataset to enable the creation of poop-aware applications.
Ultimately, our goal is for this research to contribute meaningfully to the advancement of computer vision
  and have a positive impact on society.
  
  
%\ifnonanonymous
\ifuseacknowledgement
\section{Acknowledgements}
We would would like to thank all of the dogs that produced subject matter for the dataset, all of the
contributors for helping to construct a challenging test set, and \redact{Anthony Hoogs} for several suggestions including taking the 
  third negative picture.
This work is dedicated to \redact{Bezoar}, a very weird and very good girl.

%\fi
\fi

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieeenat_fullname}
\bibliography{citations}
}


\ifuseappendix
% WARNING: do not forget to delete the supplementary pages from your submission 
\include{appendix}
%\include{neurips_2025_checklist}
\fi


\begin{comment}
    %cd $HOME/code/shitspotter
    %python -m shitspotter.cli.coco_annotation_stats $HOME/data/dvc-repos/shitspotter_dvc/data.kwcoco.json \
    %    --dst_fpath $HOME/code/shitspotter/coco_annot_stats/stats.json \
    %    --dst_dpath $HOME/code/shitspotter/coco_annot_stats

    cd $HOME/code/shitspotter
    kwcoco plot_stats \
        $HOME/data/dvc-repos/shitspotter_dvc/data.kwcoco.json \
        --dst_fpath $HOME/code/shitspotter/coco_annot_stats2/stats.json \
        --dst_dpath $HOME/code/shitspotter/coco_annot_stats2

    SeeAlso:
    ~/code/shitspotter/experiments/geowatch-experiments/run_pixel_eval_on_vali_pipeline.sh
    ~/code/shitspotter/experiments/geowatch-experiments/run_pixel_eval_on_test_pipeline.sh
    ~/code/shitspotter/experiments/geowatch-experiments/run_pixel_eval_on_train_pipeline.sh

    python ~/code/shitspotter/dev/poc/estimate_train_resources.py

    See: ./localize_figures.sh


    Best Validation Model:
        /home/joncrall/data/dvc-repos/shitspotter_expt_dvc/training/toothbrush/joncrall/ShitSpotter/runs/shitspotter_scratch_20240618_noboxes_v7/lightning_logs/version_1/checkpoints/epoch=0089-step=122940-val_loss=0.019.ckpt.pt
        # Best Rank:  33.0 pyzvffmyjcrq
        Lives in /home/joncrall/data/dvc-repos/shitspotter_expt_dvc/_shitspotter_test_evals/eval/flat/heatmap_eval/heatmap_eval_id_0f613533/pxl_eval.json heatmap_eval           pyzvffmyjcrq    0.505110     0.912509
        

    Best Test Model:
        /home/joncrall/data/dvc-repos/shitspotter_expt_dvc/training/toothbrush/joncrall/ShitSpotter/runs/shitspotter_scratch_20240618_noboxes_v6/lightning_logs/version_0/checkpoints/epoch=0073-step=101084-val_loss=0.017.ckpt.pt
        is Rank 3 on the validation dataset.
    

    cd /home/joncrall/code/shitspotter/shitspotter_dvc
    geowatch spectra --src data.kwcoco.json --workers=16 --cache_dpath=_spectra_cache --dst spectra.png --bins 64 --valid_range=0:255
    cp spectra.png ~/code/shitspotter/papers/neurips-2025/figures/spectra.png

    /home/joncrall/code/shitspotter/papers/neurips-2025
    python -m shitspotter.ipfs pull .

\end{comment}

\end{document}


